{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QMjwq6pS-kFz"
   },
   "source": [
    "# Stock NeurIPS2018 Part 2. Train\n",
    "This series is a reproduction of *the process in the paper Practical Deep Reinforcement Learning Approach for Stock Trading*. \n",
    "\n",
    "This is the second part of the NeurIPS2018 series, introducing how to use FinRL to make data into the gym form environment, and train DRL agents on it.\n",
    "\n",
    "Other demos can be found at the repo of [FinRL-Tutorials]((https://github.com/AI4Finance-Foundation/FinRL-Tutorials))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gT-zXutMgqOS"
   },
   "source": [
    "# Part 1. Install Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "D0vEcPxSJ8hI"
   },
   "outputs": [],
   "source": [
    "## install finrl library\n",
    "# !pip install git+https://github.com/AI4Finance-Foundation/FinRL.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "xt1317y2ixSS"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from stable_baselines3.common.logger import configure\n",
    "\n",
    "from rl_traders.finrl.finrl.agents.stablebaselines3.models import DRLAgent\n",
    "from rl_traders.finrl.finrl.config import INDICATORS, TRAINED_MODEL_DIR, RESULTS_DIR\n",
    "from rl_traders.finrl.finrl.main import check_and_make_directories\n",
    "from rl_traders.finrl.finrl.meta.env_stock_trading.env_stocktrading import StockTradingEnv\n",
    "\n",
    "check_and_make_directories([TRAINED_MODEL_DIR])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aWrSrQv3i0Ng"
   },
   "source": [
    "# Part 2. Build A Market Environment in OpenAI Gym-style"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wiHhM2U-XBMZ"
   },
   "source": [
    "![rl_diagram_transparent_bg.png](data:image/png;base64,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)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LeneTRdyZDvy"
   },
   "source": [
    "The core element in reinforcement learning are **agent** and **environment**. You can understand RL as the following process: \n",
    "\n",
    "The agent is active in a world, which is the environment. It observe its current condition as a **state**, and is allowed to do certain **actions**. After the agent execute an action, it will arrive at a new state. At the same time, the environment will have feedback to the agent called **reward**, a numerical signal that tells how good or bad the new state is. As the figure above, agent and environment will keep doing this interaction.\n",
    "\n",
    "The goal of agent is to get as much cumulative reward as possible. Reinforcement learning is the method that agent learns to improve its behavior and achieve that goal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w3H88JXkI93v"
   },
   "source": [
    "To achieve this in Python, we follow the OpenAI gym style to build the stock data into environment.\n",
    "\n",
    "state-action-reward are specified as follows:\n",
    "\n",
    "* **State s**: The state space represents an agent's perception of the market environment. Just like a human trader analyzing various information, here our agent passively observes the price data and technical indicators based on the past data. It will learn by interacting with the market environment (usually by replaying historical data).\n",
    "\n",
    "* **Action a**: The action space includes allowed actions that an agent can take at each state. For example, a ∈ {−1, 0, 1}, where −1, 0, 1 represent\n",
    "selling, holding, and buying. When an action operates multiple shares, a ∈{−k, ..., −1, 0, 1, ..., k}, e.g.. \"Buy 10 shares of AAPL\" or \"Sell 10 shares of AAPL\" are 10 or −10, respectively\n",
    "\n",
    "* **Reward function r(s, a, s′)**: Reward is an incentive for an agent to learn a better policy. For example, it can be the change of the portfolio value when taking a at state s and arriving at new state s',  i.e., r(s, a, s′) = v′ − v, where v′ and v represent the portfolio values at state s′ and s, respectively\n",
    "\n",
    "\n",
    "**Market environment**: 30 constituent stocks of Dow Jones Industrial Average (DJIA) index. Accessed at the starting date of the testing period."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SKyZejI0fmp1"
   },
   "source": [
    "## Read data\n",
    "\n",
    "We first read the .csv file of our training data into dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "mFCP1YEhi6oi"
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('train_data.csv')\n",
    "\n",
    "# If you are not using the data generated from part 1 of this tutorial, make sure \n",
    "# it has the columns and index in the form that could be make into the environment. \n",
    "# Then you can comment and skip the following two lines.\n",
    "train = train.set_index(train.columns[0])\n",
    "train.index.names = ['']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yw95ZMicgEyi"
   },
   "source": [
    "## Construct the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5WZ6-9q2gq9S"
   },
   "source": [
    "Calculate and specify the parameters we need for constructing the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7T3DZPoaIm8k",
    "outputId": "4817e063-400a-416e-f8f2-4b1c4d9c8408"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stock Dimension: 1, State Space: 11\n"
     ]
    }
   ],
   "source": [
    "stock_dimension = len(train.tic.unique())\n",
    "state_space = 1 + 2*stock_dimension + len(INDICATORS)*stock_dimension\n",
    "print(f\"Stock Dimension: {stock_dimension}, State Space: {state_space}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "WsOLoeNcJF8Q"
   },
   "outputs": [],
   "source": [
    "buy_cost_list = sell_cost_list = [0.001] * stock_dimension\n",
    "num_stock_shares = [0] * stock_dimension\n",
    "\n",
    "env_kwargs = {\n",
    "    \"hmax\": 100,\n",
    "    \"initial_amount\": 1000000,\n",
    "    \"num_stock_shares\": num_stock_shares,\n",
    "    \"buy_cost_pct\": buy_cost_list,\n",
    "    \"sell_cost_pct\": sell_cost_list,\n",
    "    \"state_space\": state_space,\n",
    "    \"stock_dim\": stock_dimension,\n",
    "    \"tech_indicator_list\": INDICATORS,\n",
    "    \"action_space\": stock_dimension,\n",
    "    \"reward_scaling\": 1e-4\n",
    "}\n",
    "\n",
    "\n",
    "e_train_gym = StockTradingEnv(df = train, **env_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7We-q73jjaFQ"
   },
   "source": [
    "## Environment for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aS-SHiGRJK-4",
    "outputId": "a733ecdf-d857-40f5-b399-4325c7ead299"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv'>\n"
     ]
    }
   ],
   "source": [
    "env_train, _ = e_train_gym.get_sb_env()\n",
    "print(type(env_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HMNR5nHjh1iz"
   },
   "source": [
    "# Part 3: Train DRL Agents\n",
    "* Here, the DRL algorithms are from **[Stable Baselines 3](https://stable-baselines3.readthedocs.io/en/master/)**. It's a library that implemented popular DRL algorithms using pytorch, succeeding to its old version: Stable Baselines.\n",
    "* Users are also encouraged to try **[ElegantRL](https://github.com/AI4Finance-Foundation/ElegantRL)** and **[Ray RLlib](https://github.com/ray-project/ray)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "364PsqckttcQ"
   },
   "outputs": [],
   "source": [
    "agent = DRLAgent(env = env_train)\n",
    "\n",
    "# Set the corresponding values to 'True' for the algorithms that you want to use\n",
    "if_using_a2c = True\n",
    "if_using_ddpg = True\n",
    "if_using_ppo = True\n",
    "if_using_td3 = True\n",
    "if_using_sac = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YDmqOyF9h1iz"
   },
   "source": [
    "## Agent Training: 5 algorithms (A2C, DDPG, PPO, TD3, SAC)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uijiWgkuh1jB"
   },
   "source": [
    "### Agent 1: A2C\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GUCnkn-HIbmj",
    "outputId": "2794a094-a916-448c-ead1-6e20184dde2a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_steps': 5, 'ent_coef': 0.01, 'learning_rate': 0.0007}\n",
      "Using cuda device\n",
      "Logging to results/a2c\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Development\\Anaconda3\\envs\\trading\\lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:150: UserWarning: You are trying to run A2C on the GPU, but it is primarily intended to run on the CPU when not using a CNN policy (you are using ActorCriticPolicy which should be a MlpPolicy). See https://github.com/DLR-RM/stable-baselines3/issues/1245 for more info. You can pass `device='cpu'` or `export CUDA_VISIBLE_DEVICES=` to force using the CPU.Note: The model will train, but the GPU utilization will be poor and the training might take longer than on CPU.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "agent = DRLAgent(env = env_train)\n",
    "model_a2c = agent.get_model(\"a2c\")\n",
    "\n",
    "if if_using_a2c:\n",
    "  # set up logger\n",
    "  tmp_path = RESULTS_DIR + '/a2c'\n",
    "  new_logger_a2c = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n",
    "  # Set new logger\n",
    "  model_a2c.set_logger(new_logger_a2c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0GVpkWGqH4-D",
    "outputId": "f29cf145-e3b5-4e59-f64d-5921462a8f81"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 173        |\n",
      "|    iterations         | 100        |\n",
      "|    time_elapsed       | 2          |\n",
      "|    total_timesteps    | 500        |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.41      |\n",
      "|    explained_variance | -0.0927    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 99         |\n",
      "|    policy_loss        | 0.267      |\n",
      "|    reward             | -1.2779773 |\n",
      "|    std                | 0.988      |\n",
      "|    value_loss         | 6.8        |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 176       |\n",
      "|    iterations         | 200       |\n",
      "|    time_elapsed       | 5         |\n",
      "|    total_timesteps    | 1000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.41     |\n",
      "|    explained_variance | -0.242    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 199       |\n",
      "|    policy_loss        | -5.13     |\n",
      "|    reward             | 2.8983097 |\n",
      "|    std                | 0.995     |\n",
      "|    value_loss         | 20.1      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 178        |\n",
      "|    iterations         | 300        |\n",
      "|    time_elapsed       | 8          |\n",
      "|    total_timesteps    | 1500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.42      |\n",
      "|    explained_variance | -0.0106    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 299        |\n",
      "|    policy_loss        | -5.85      |\n",
      "|    reward             | -2.1737792 |\n",
      "|    std                | 0.999      |\n",
      "|    value_loss         | 34.1       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 183        |\n",
      "|    iterations         | 400        |\n",
      "|    time_elapsed       | 10         |\n",
      "|    total_timesteps    | 2000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.41      |\n",
      "|    explained_variance | 0.29       |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 399        |\n",
      "|    policy_loss        | -2.61      |\n",
      "|    reward             | 0.21969247 |\n",
      "|    std                | 0.992      |\n",
      "|    value_loss         | 4.63       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 183         |\n",
      "|    iterations         | 500         |\n",
      "|    time_elapsed       | 13          |\n",
      "|    total_timesteps    | 2500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -1.43       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 499         |\n",
      "|    policy_loss        | -2.87       |\n",
      "|    reward             | -0.18639773 |\n",
      "|    std                | 1.01        |\n",
      "|    value_loss         | 2.71        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 187        |\n",
      "|    iterations         | 600        |\n",
      "|    time_elapsed       | 16         |\n",
      "|    total_timesteps    | 3000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.42      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 599        |\n",
      "|    policy_loss        | -0.483     |\n",
      "|    reward             | 0.15231416 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 0.168      |\n",
      "--------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                 |               |\n",
      "|    fps                | 190           |\n",
      "|    iterations         | 700           |\n",
      "|    time_elapsed       | 18            |\n",
      "|    total_timesteps    | 3500          |\n",
      "| train/                |               |\n",
      "|    entropy_loss       | -1.42         |\n",
      "|    explained_variance | 0             |\n",
      "|    learning_rate      | 0.0007        |\n",
      "|    n_updates          | 699           |\n",
      "|    policy_loss        | -0.0142       |\n",
      "|    reward             | -0.0010867318 |\n",
      "|    std                | 0.997         |\n",
      "|    value_loss         | 0.000167      |\n",
      "-----------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 192        |\n",
      "|    iterations         | 800        |\n",
      "|    time_elapsed       | 20         |\n",
      "|    total_timesteps    | 4000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.43      |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 799        |\n",
      "|    policy_loss        | 0.201      |\n",
      "|    reward             | 0.10135879 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 0.00983    |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 196         |\n",
      "|    iterations         | 900         |\n",
      "|    time_elapsed       | 22          |\n",
      "|    total_timesteps    | 4500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -1.46       |\n",
      "|    explained_variance | 1.19e-07    |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 899         |\n",
      "|    policy_loss        | 0.831       |\n",
      "|    reward             | -0.16462412 |\n",
      "|    std                | 1.05        |\n",
      "|    value_loss         | 0.565       |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 199         |\n",
      "|    iterations         | 1000        |\n",
      "|    time_elapsed       | 25          |\n",
      "|    total_timesteps    | 5000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -1.47       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 999         |\n",
      "|    policy_loss        | -0.323      |\n",
      "|    reward             | -0.32136342 |\n",
      "|    std                | 1.05        |\n",
      "|    value_loss         | 0.107       |\n",
      "---------------------------------------\n",
      "------------------------------------------\n",
      "| time/                 |                |\n",
      "|    fps                | 202            |\n",
      "|    iterations         | 1100           |\n",
      "|    time_elapsed       | 27             |\n",
      "|    total_timesteps    | 5500           |\n",
      "| train/                |                |\n",
      "|    entropy_loss       | -1.47          |\n",
      "|    explained_variance | -1.19e-07      |\n",
      "|    learning_rate      | 0.0007         |\n",
      "|    n_updates          | 1099           |\n",
      "|    policy_loss        | 0.107          |\n",
      "|    reward             | -0.00070417376 |\n",
      "|    std                | 1.06           |\n",
      "|    value_loss         | 0.0114         |\n",
      "------------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 205       |\n",
      "|    iterations         | 1200      |\n",
      "|    time_elapsed       | 29        |\n",
      "|    total_timesteps    | 6000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.48     |\n",
      "|    explained_variance | -0.263    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1199      |\n",
      "|    policy_loss        | 3.57      |\n",
      "|    reward             | 0.6305076 |\n",
      "|    std                | 1.06      |\n",
      "|    value_loss         | 17.2      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 206         |\n",
      "|    iterations         | 1300        |\n",
      "|    time_elapsed       | 31          |\n",
      "|    total_timesteps    | 6500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -1.48       |\n",
      "|    explained_variance | -1.19e-07   |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1299        |\n",
      "|    policy_loss        | -0.0966     |\n",
      "|    reward             | -0.27548364 |\n",
      "|    std                | 1.07        |\n",
      "|    value_loss         | 1.63        |\n",
      "---------------------------------------\n",
      "day: 754, episode: 10\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1315103.18\n",
      "total_reward: 315103.18\n",
      "total_cost: 10500.12\n",
      "total_trades: 745\n",
      "Sharpe: 0.512\n",
      "=================================\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 207          |\n",
      "|    iterations         | 1400         |\n",
      "|    time_elapsed       | 33           |\n",
      "|    total_timesteps    | 7000         |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -1.49        |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 1399         |\n",
      "|    policy_loss        | -0.213       |\n",
      "|    reward             | -0.005845839 |\n",
      "|    std                | 1.08         |\n",
      "|    value_loss         | 0.043        |\n",
      "----------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 207        |\n",
      "|    iterations         | 1500       |\n",
      "|    time_elapsed       | 36         |\n",
      "|    total_timesteps    | 7500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.49      |\n",
      "|    explained_variance | 0.157      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1499       |\n",
      "|    policy_loss        | 2.77       |\n",
      "|    reward             | -3.0546489 |\n",
      "|    std                | 1.08       |\n",
      "|    value_loss         | 4.97       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 206        |\n",
      "|    iterations         | 1600       |\n",
      "|    time_elapsed       | 38         |\n",
      "|    total_timesteps    | 8000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.51      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1599       |\n",
      "|    policy_loss        | 0.72       |\n",
      "|    reward             | 0.37125602 |\n",
      "|    std                | 1.09       |\n",
      "|    value_loss         | 0.357      |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 206       |\n",
      "|    iterations         | 1700      |\n",
      "|    time_elapsed       | 41        |\n",
      "|    total_timesteps    | 8500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.5      |\n",
      "|    explained_variance | 5.96e-08  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1699      |\n",
      "|    policy_loss        | 1.23      |\n",
      "|    reward             | -0.710891 |\n",
      "|    std                | 1.09      |\n",
      "|    value_loss         | 0.646     |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 207       |\n",
      "|    iterations         | 1800      |\n",
      "|    time_elapsed       | 43        |\n",
      "|    total_timesteps    | 9000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.5      |\n",
      "|    explained_variance | 1.19e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1799      |\n",
      "|    policy_loss        | 0.138     |\n",
      "|    reward             | -0.640401 |\n",
      "|    std                | 1.09      |\n",
      "|    value_loss         | 0.0487    |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 208       |\n",
      "|    iterations         | 1900      |\n",
      "|    time_elapsed       | 45        |\n",
      "|    total_timesteps    | 9500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.5      |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1899      |\n",
      "|    policy_loss        | 1.26      |\n",
      "|    reward             | 0.3868062 |\n",
      "|    std                | 1.09      |\n",
      "|    value_loss         | 0.84      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 208        |\n",
      "|    iterations         | 2000       |\n",
      "|    time_elapsed       | 47         |\n",
      "|    total_timesteps    | 10000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.49      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1999       |\n",
      "|    policy_loss        | -0.212     |\n",
      "|    reward             | 0.35558474 |\n",
      "|    std                | 1.07       |\n",
      "|    value_loss         | 0.712      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 208        |\n",
      "|    iterations         | 2100       |\n",
      "|    time_elapsed       | 50         |\n",
      "|    total_timesteps    | 10500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.5       |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 2099       |\n",
      "|    policy_loss        | -0.139     |\n",
      "|    reward             | -1.4353294 |\n",
      "|    std                | 1.08       |\n",
      "|    value_loss         | 4.7        |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 208      |\n",
      "|    iterations         | 2200     |\n",
      "|    time_elapsed       | 52       |\n",
      "|    total_timesteps    | 11000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.49    |\n",
      "|    explained_variance | -0.0435  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 2199     |\n",
      "|    policy_loss        | 1.16     |\n",
      "|    reward             | 1.796591 |\n",
      "|    std                | 1.07     |\n",
      "|    value_loss         | 3.96     |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 207        |\n",
      "|    iterations         | 2300       |\n",
      "|    time_elapsed       | 55         |\n",
      "|    total_timesteps    | 11500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.49      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 2299       |\n",
      "|    policy_loss        | 5.34       |\n",
      "|    reward             | 0.20473388 |\n",
      "|    std                | 1.07       |\n",
      "|    value_loss         | 8.25       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 206       |\n",
      "|    iterations         | 2400      |\n",
      "|    time_elapsed       | 58        |\n",
      "|    total_timesteps    | 12000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.49     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 2399      |\n",
      "|    policy_loss        | -0.718    |\n",
      "|    reward             | -3.405902 |\n",
      "|    std                | 1.07      |\n",
      "|    value_loss         | 1.26      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 206         |\n",
      "|    iterations         | 2500        |\n",
      "|    time_elapsed       | 60          |\n",
      "|    total_timesteps    | 12500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -1.49       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 2499        |\n",
      "|    policy_loss        | -0.436      |\n",
      "|    reward             | -0.15559155 |\n",
      "|    std                | 1.08        |\n",
      "|    value_loss         | 0.164       |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 206         |\n",
      "|    iterations         | 2600        |\n",
      "|    time_elapsed       | 63          |\n",
      "|    total_timesteps    | 13000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -1.51       |\n",
      "|    explained_variance | -1.19e-07   |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 2599        |\n",
      "|    policy_loss        | -0.161      |\n",
      "|    reward             | 0.024013761 |\n",
      "|    std                | 1.09        |\n",
      "|    value_loss         | 0.0232      |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 204       |\n",
      "|    iterations         | 2700      |\n",
      "|    time_elapsed       | 66        |\n",
      "|    total_timesteps    | 13500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.53     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 2699      |\n",
      "|    policy_loss        | -1.81     |\n",
      "|    reward             | 0.5466397 |\n",
      "|    std                | 1.11      |\n",
      "|    value_loss         | 1.64      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 202        |\n",
      "|    iterations         | 2800       |\n",
      "|    time_elapsed       | 68         |\n",
      "|    total_timesteps    | 14000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.53      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 2799       |\n",
      "|    policy_loss        | 0.113      |\n",
      "|    reward             | 0.24116391 |\n",
      "|    std                | 1.12       |\n",
      "|    value_loss         | 0.0804     |\n",
      "--------------------------------------\n",
      "day: 754, episode: 20\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 999844.18\n",
      "total_reward: -155.82\n",
      "total_cost: 9423.81\n",
      "total_trades: 656\n",
      "Sharpe: 0.006\n",
      "=================================\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 201         |\n",
      "|    iterations         | 2900        |\n",
      "|    time_elapsed       | 71          |\n",
      "|    total_timesteps    | 14500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -1.53       |\n",
      "|    explained_variance | 2.38e-07    |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 2899        |\n",
      "|    policy_loss        | -0.418      |\n",
      "|    reward             | -0.26582205 |\n",
      "|    std                | 1.12        |\n",
      "|    value_loss         | 0.216       |\n",
      "---------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 200      |\n",
      "|    iterations         | 3000     |\n",
      "|    time_elapsed       | 74       |\n",
      "|    total_timesteps    | 15000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.53    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 2999     |\n",
      "|    policy_loss        | -0.0971  |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.11     |\n",
      "|    value_loss         | 0.00617  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 200      |\n",
      "|    iterations         | 3100     |\n",
      "|    time_elapsed       | 77       |\n",
      "|    total_timesteps    | 15500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.54    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 3099     |\n",
      "|    policy_loss        | -0.0044  |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.13     |\n",
      "|    value_loss         | 1.16e-05 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 199      |\n",
      "|    iterations         | 3200     |\n",
      "|    time_elapsed       | 80       |\n",
      "|    total_timesteps    | 16000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.55    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 3199     |\n",
      "|    policy_loss        | -0.00149 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.15     |\n",
      "|    value_loss         | 1.98e-06 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 199      |\n",
      "|    iterations         | 3300     |\n",
      "|    time_elapsed       | 82       |\n",
      "|    total_timesteps    | 16500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.58    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 3299     |\n",
      "|    policy_loss        | 0.000321 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.17     |\n",
      "|    value_loss         | 5.08e-08 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 199       |\n",
      "|    iterations         | 3400      |\n",
      "|    time_elapsed       | 85        |\n",
      "|    total_timesteps    | 17000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.6      |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 3399      |\n",
      "|    policy_loss        | 0.000669  |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 1.2       |\n",
      "|    value_loss         | 3.77e-07  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 199       |\n",
      "|    iterations         | 3500      |\n",
      "|    time_elapsed       | 87        |\n",
      "|    total_timesteps    | 17500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.64     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 3499      |\n",
      "|    policy_loss        | 2e-05     |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 1.25      |\n",
      "|    value_loss         | 1.89e-10  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 199      |\n",
      "|    iterations         | 3600     |\n",
      "|    time_elapsed       | 90       |\n",
      "|    total_timesteps    | 18000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.67    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 3599     |\n",
      "|    policy_loss        | -0.015   |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.28     |\n",
      "|    value_loss         | 0.000121 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 199       |\n",
      "|    iterations         | 3700      |\n",
      "|    time_elapsed       | 92        |\n",
      "|    total_timesteps    | 18500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.7      |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 3699      |\n",
      "|    policy_loss        | -0.00744  |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 1.32      |\n",
      "|    value_loss         | 1.59e-05  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 198      |\n",
      "|    iterations         | 3800     |\n",
      "|    time_elapsed       | 95       |\n",
      "|    total_timesteps    | 19000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.73    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 3799     |\n",
      "|    policy_loss        | 0.00868  |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.37     |\n",
      "|    value_loss         | 4.44e-05 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 198      |\n",
      "|    iterations         | 3900     |\n",
      "|    time_elapsed       | 98       |\n",
      "|    total_timesteps    | 19500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.77    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 3899     |\n",
      "|    policy_loss        | 0.00133  |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.42     |\n",
      "|    value_loss         | 9.13e-07 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 198      |\n",
      "|    iterations         | 4000     |\n",
      "|    time_elapsed       | 100      |\n",
      "|    total_timesteps    | 20000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.82    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 3999     |\n",
      "|    policy_loss        | -0.0051  |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.49     |\n",
      "|    value_loss         | 6.85e-06 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 197       |\n",
      "|    iterations         | 4100      |\n",
      "|    time_elapsed       | 103       |\n",
      "|    total_timesteps    | 20500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.85     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 4099      |\n",
      "|    policy_loss        | -0.000866 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 1.54      |\n",
      "|    value_loss         | 2.83e-07  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 196       |\n",
      "|    iterations         | 4200      |\n",
      "|    time_elapsed       | 106       |\n",
      "|    total_timesteps    | 21000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.9      |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 4199      |\n",
      "|    policy_loss        | -3.29e-05 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 1.61      |\n",
      "|    value_loss         | 4.47e-10  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 196      |\n",
      "|    iterations         | 4300     |\n",
      "|    time_elapsed       | 109      |\n",
      "|    total_timesteps    | 21500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.94    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 4299     |\n",
      "|    policy_loss        | -0.0242  |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.69     |\n",
      "|    value_loss         | 0.00012  |\n",
      "------------------------------------\n",
      "day: 754, episode: 30\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1000623.29\n",
      "total_reward: 623.29\n",
      "total_cost: 509.00\n",
      "total_trades: 47\n",
      "Sharpe: 0.337\n",
      "=================================\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 195      |\n",
      "|    iterations         | 4400     |\n",
      "|    time_elapsed       | 112      |\n",
      "|    total_timesteps    | 22000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.99    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 4399     |\n",
      "|    policy_loss        | 1.84e-05 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.77     |\n",
      "|    value_loss         | 1.1e-10  |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 194       |\n",
      "|    iterations         | 4500      |\n",
      "|    time_elapsed       | 115       |\n",
      "|    total_timesteps    | 22500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.03     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 4499      |\n",
      "|    policy_loss        | 0.0154    |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 1.84      |\n",
      "|    value_loss         | 8.43e-05  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 193      |\n",
      "|    iterations         | 4600     |\n",
      "|    time_elapsed       | 118      |\n",
      "|    total_timesteps    | 23000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.07    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 4599     |\n",
      "|    policy_loss        | -0.00304 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.92     |\n",
      "|    value_loss         | 3.82e-06 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 193      |\n",
      "|    iterations         | 4700     |\n",
      "|    time_elapsed       | 121      |\n",
      "|    total_timesteps    | 23500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.11    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 4699     |\n",
      "|    policy_loss        | 1.5e-05  |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 2        |\n",
      "|    value_loss         | 7.67e-11 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 195      |\n",
      "|    iterations         | 4800     |\n",
      "|    time_elapsed       | 122      |\n",
      "|    total_timesteps    | 24000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.17    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 4799     |\n",
      "|    policy_loss        | -2.9e-06 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 2.11     |\n",
      "|    value_loss         | 2.23e-12 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 197       |\n",
      "|    iterations         | 4900      |\n",
      "|    time_elapsed       | 124       |\n",
      "|    total_timesteps    | 24500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.21     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 4899      |\n",
      "|    policy_loss        | -1.43e-06 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 2.21      |\n",
      "|    value_loss         | 2.75e-13  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 199      |\n",
      "|    iterations         | 5000     |\n",
      "|    time_elapsed       | 125      |\n",
      "|    total_timesteps    | 25000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.25    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 4999     |\n",
      "|    policy_loss        | -0.00138 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 2.3      |\n",
      "|    value_loss         | 6.3e-07  |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 201       |\n",
      "|    iterations         | 5100      |\n",
      "|    time_elapsed       | 126       |\n",
      "|    total_timesteps    | 25500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.28     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 5099      |\n",
      "|    policy_loss        | 0.0165    |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 2.38      |\n",
      "|    value_loss         | 7.58e-05  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 203      |\n",
      "|    iterations         | 5200     |\n",
      "|    time_elapsed       | 128      |\n",
      "|    total_timesteps    | 26000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.32    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 5199     |\n",
      "|    policy_loss        | 0.00515  |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 2.46     |\n",
      "|    value_loss         | 4.2e-06  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 204      |\n",
      "|    iterations         | 5300     |\n",
      "|    time_elapsed       | 129      |\n",
      "|    total_timesteps    | 26500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.35    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 5299     |\n",
      "|    policy_loss        | 0.000608 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 2.54     |\n",
      "|    value_loss         | 1.05e-07 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 206      |\n",
      "|    iterations         | 5400     |\n",
      "|    time_elapsed       | 130      |\n",
      "|    total_timesteps    | 27000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.37    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 5399     |\n",
      "|    policy_loss        | 0.000326 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 2.59     |\n",
      "|    value_loss         | 1.96e-08 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 208       |\n",
      "|    iterations         | 5500      |\n",
      "|    time_elapsed       | 131       |\n",
      "|    total_timesteps    | 27500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.39     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 5499      |\n",
      "|    policy_loss        | 0.0018    |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 2.64      |\n",
      "|    value_loss         | 9.42e-07  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 209       |\n",
      "|    iterations         | 5600      |\n",
      "|    time_elapsed       | 133       |\n",
      "|    total_timesteps    | 28000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.43     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 5599      |\n",
      "|    policy_loss        | -0.00207  |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 2.76      |\n",
      "|    value_loss         | 1.24e-06  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 208       |\n",
      "|    iterations         | 5700      |\n",
      "|    time_elapsed       | 136       |\n",
      "|    total_timesteps    | 28500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.47     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 5699      |\n",
      "|    policy_loss        | 0.000511  |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 2.87      |\n",
      "|    value_loss         | 6.2e-08   |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 207      |\n",
      "|    iterations         | 5800     |\n",
      "|    time_elapsed       | 139      |\n",
      "|    total_timesteps    | 29000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.5     |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 5799     |\n",
      "|    policy_loss        | 0.00274  |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 2.95     |\n",
      "|    value_loss         | 1.28e-06 |\n",
      "------------------------------------\n",
      "day: 754, episode: 40\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 998374.64\n",
      "total_reward: -1625.36\n",
      "total_cost: 576.86\n",
      "total_trades: 38\n",
      "Sharpe: -0.905\n",
      "=================================\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 207       |\n",
      "|    iterations         | 5900      |\n",
      "|    time_elapsed       | 142       |\n",
      "|    total_timesteps    | 29500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.54     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 5899      |\n",
      "|    policy_loss        | -0.000897 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 3.07      |\n",
      "|    value_loss         | 1.83e-07  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 206       |\n",
      "|    iterations         | 6000      |\n",
      "|    time_elapsed       | 145       |\n",
      "|    total_timesteps    | 30000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.59     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 5999      |\n",
      "|    policy_loss        | -1.25e-08 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 3.22      |\n",
      "|    value_loss         | 4.25e-15  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 206      |\n",
      "|    iterations         | 6100     |\n",
      "|    time_elapsed       | 147      |\n",
      "|    total_timesteps    | 30500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.64    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 6099     |\n",
      "|    policy_loss        | -0.00428 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 3.4      |\n",
      "|    value_loss         | 3.79e-06 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 205       |\n",
      "|    iterations         | 6200      |\n",
      "|    time_elapsed       | 150       |\n",
      "|    total_timesteps    | 31000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.68     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 6199      |\n",
      "|    policy_loss        | -0.000419 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 3.53      |\n",
      "|    value_loss         | 4.02e-08  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 205       |\n",
      "|    iterations         | 6300      |\n",
      "|    time_elapsed       | 153       |\n",
      "|    total_timesteps    | 31500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.73     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 6299      |\n",
      "|    policy_loss        | -0.0236   |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 3.72      |\n",
      "|    value_loss         | 9.75e-05  |\n",
      "-------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 205          |\n",
      "|    iterations         | 6400         |\n",
      "|    time_elapsed       | 155          |\n",
      "|    total_timesteps    | 32000        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -2.78        |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 6399         |\n",
      "|    policy_loss        | -0.00265     |\n",
      "|    reward             | -0.013293373 |\n",
      "|    std                | 3.9          |\n",
      "|    value_loss         | 1.19e-06     |\n",
      "----------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 206       |\n",
      "|    iterations         | 6500      |\n",
      "|    time_elapsed       | 157       |\n",
      "|    total_timesteps    | 32500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.82     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 6499      |\n",
      "|    policy_loss        | -0.000851 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 4.06      |\n",
      "|    value_loss         | 1.73e-07  |\n",
      "-------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                 |               |\n",
      "|    fps                | 207           |\n",
      "|    iterations         | 6600          |\n",
      "|    time_elapsed       | 158           |\n",
      "|    total_timesteps    | 33000         |\n",
      "| train/                |               |\n",
      "|    entropy_loss       | -2.87         |\n",
      "|    explained_variance | 0             |\n",
      "|    learning_rate      | 0.0007        |\n",
      "|    n_updates          | 6599          |\n",
      "|    policy_loss        | -0.00822      |\n",
      "|    reward             | -0.0033054724 |\n",
      "|    std                | 4.25          |\n",
      "|    value_loss         | 1.04e-05      |\n",
      "-----------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 209      |\n",
      "|    iterations         | 6700     |\n",
      "|    time_elapsed       | 160      |\n",
      "|    total_timesteps    | 33500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.88    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 6699     |\n",
      "|    policy_loss        | 0.00133  |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 4.3      |\n",
      "|    value_loss         | 2.76e-07 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 210      |\n",
      "|    iterations         | 6800     |\n",
      "|    time_elapsed       | 161      |\n",
      "|    total_timesteps    | 34000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.9     |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 6799     |\n",
      "|    policy_loss        | -0.00491 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 4.4      |\n",
      "|    value_loss         | 3.81e-06 |\n",
      "------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                 |               |\n",
      "|    fps                | 212           |\n",
      "|    iterations         | 6900          |\n",
      "|    time_elapsed       | 162           |\n",
      "|    total_timesteps    | 34500         |\n",
      "| train/                |               |\n",
      "|    entropy_loss       | -2.92         |\n",
      "|    explained_variance | 5.96e-08      |\n",
      "|    learning_rate      | 0.0007        |\n",
      "|    n_updates          | 6899          |\n",
      "|    policy_loss        | 0.00433       |\n",
      "|    reward             | -0.0032496213 |\n",
      "|    std                | 4.51          |\n",
      "|    value_loss         | 2.25e-06      |\n",
      "-----------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 213      |\n",
      "|    iterations         | 7000     |\n",
      "|    time_elapsed       | 163      |\n",
      "|    total_timesteps    | 35000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.95    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 6999     |\n",
      "|    policy_loss        | -0.00483 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 4.64     |\n",
      "|    value_loss         | 3.12e-06 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 214       |\n",
      "|    iterations         | 7100      |\n",
      "|    time_elapsed       | 165       |\n",
      "|    total_timesteps    | 35500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.98     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 7099      |\n",
      "|    policy_loss        | -0.000533 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 4.78      |\n",
      "|    value_loss         | 4.11e-08  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 216      |\n",
      "|    iterations         | 7200     |\n",
      "|    time_elapsed       | 166      |\n",
      "|    total_timesteps    | 36000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -3.01    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 7199     |\n",
      "|    policy_loss        | -0.00691 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 4.92     |\n",
      "|    value_loss         | 1.11e-05 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 217      |\n",
      "|    iterations         | 7300     |\n",
      "|    time_elapsed       | 167      |\n",
      "|    total_timesteps    | 36500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -3.04    |\n",
      "|    explained_variance | 5.96e-08 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 7299     |\n",
      "|    policy_loss        | 0.00538  |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 5.06     |\n",
      "|    value_loss         | 3.99e-06 |\n",
      "------------------------------------\n",
      "day: 754, episode: 50\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 997178.57\n",
      "total_reward: -2821.43\n",
      "total_cost: 1230.97\n",
      "total_trades: 68\n",
      "Sharpe: -0.603\n",
      "=================================\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 218      |\n",
      "|    iterations         | 7400     |\n",
      "|    time_elapsed       | 169      |\n",
      "|    total_timesteps    | 37000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -3.07    |\n",
      "|    explained_variance | nan      |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 7399     |\n",
      "|    policy_loss        | 0.0377   |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 5.2      |\n",
      "|    value_loss         | 0.000117 |\n",
      "------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                 |               |\n",
      "|    fps                | 220           |\n",
      "|    iterations         | 7500          |\n",
      "|    time_elapsed       | 170           |\n",
      "|    total_timesteps    | 37500         |\n",
      "| train/                |               |\n",
      "|    entropy_loss       | -3.1          |\n",
      "|    explained_variance | 1.19e-07      |\n",
      "|    learning_rate      | 0.0007        |\n",
      "|    n_updates          | 7499          |\n",
      "|    policy_loss        | -0.0036       |\n",
      "|    reward             | -0.0028517859 |\n",
      "|    std                | 5.37          |\n",
      "|    value_loss         | 3.49e-06      |\n",
      "-----------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 221      |\n",
      "|    iterations         | 7600     |\n",
      "|    time_elapsed       | 171      |\n",
      "|    total_timesteps    | 38000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -3.13    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 7599     |\n",
      "|    policy_loss        | -0.0014  |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 5.55     |\n",
      "|    value_loss         | 8.69e-07 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 222      |\n",
      "|    iterations         | 7700     |\n",
      "|    time_elapsed       | 172      |\n",
      "|    total_timesteps    | 38500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -3.18    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 7699     |\n",
      "|    policy_loss        | -0.016   |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 5.84     |\n",
      "|    value_loss         | 3.8e-05  |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 224       |\n",
      "|    iterations         | 7800      |\n",
      "|    time_elapsed       | 174       |\n",
      "|    total_timesteps    | 39000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -3.22     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 7799      |\n",
      "|    policy_loss        | -0.00322  |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 6.04      |\n",
      "|    value_loss         | 4.74e-05  |\n",
      "-------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 225          |\n",
      "|    iterations         | 7900         |\n",
      "|    time_elapsed       | 175          |\n",
      "|    total_timesteps    | 39500        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -3.24        |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 7899         |\n",
      "|    policy_loss        | 0.00817      |\n",
      "|    reward             | -0.001956243 |\n",
      "|    std                | 6.16         |\n",
      "|    value_loss         | 5.73e-06     |\n",
      "----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                 |                |\n",
      "|    fps                | 226            |\n",
      "|    iterations         | 8000           |\n",
      "|    time_elapsed       | 176            |\n",
      "|    total_timesteps    | 40000          |\n",
      "| train/                |                |\n",
      "|    entropy_loss       | -3.27          |\n",
      "|    explained_variance | 0              |\n",
      "|    learning_rate      | 0.0007         |\n",
      "|    n_updates          | 7999           |\n",
      "|    policy_loss        | 0.0012         |\n",
      "|    reward             | -0.00021562378 |\n",
      "|    std                | 6.38           |\n",
      "|    value_loss         | 1.48e-07       |\n",
      "------------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 227          |\n",
      "|    iterations         | 8100         |\n",
      "|    time_elapsed       | 178          |\n",
      "|    total_timesteps    | 40500        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -3.32        |\n",
      "|    explained_variance | 5.96e-08     |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 8099         |\n",
      "|    policy_loss        | -0.0105      |\n",
      "|    reward             | -0.002920865 |\n",
      "|    std                | 6.68         |\n",
      "|    value_loss         | 1.53e-05     |\n",
      "----------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 228       |\n",
      "|    iterations         | 8200      |\n",
      "|    time_elapsed       | 179       |\n",
      "|    total_timesteps    | 41000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -3.35     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 8199      |\n",
      "|    policy_loss        | 0.518     |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 6.9       |\n",
      "|    value_loss         | 0.0246    |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 229         |\n",
      "|    iterations         | 8300        |\n",
      "|    time_elapsed       | 180         |\n",
      "|    total_timesteps    | 41500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -3.39       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 8299        |\n",
      "|    policy_loss        | 0.00852     |\n",
      "|    reward             | -0.02477469 |\n",
      "|    std                | 7.21        |\n",
      "|    value_loss         | 7.63e-06    |\n",
      "---------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 230      |\n",
      "|    iterations         | 8400     |\n",
      "|    time_elapsed       | 181      |\n",
      "|    total_timesteps    | 42000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -3.43    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 8399     |\n",
      "|    policy_loss        | -0.00838 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 7.44     |\n",
      "|    value_loss         | 6.43e-06 |\n",
      "------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 231         |\n",
      "|    iterations         | 8500        |\n",
      "|    time_elapsed       | 183         |\n",
      "|    total_timesteps    | 42500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -3.46       |\n",
      "|    explained_variance | 1.19e-07    |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 8499        |\n",
      "|    policy_loss        | 0.11        |\n",
      "|    reward             | 0.045026176 |\n",
      "|    std                | 7.69        |\n",
      "|    value_loss         | 0.00116     |\n",
      "---------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 232      |\n",
      "|    iterations         | 8600     |\n",
      "|    time_elapsed       | 184      |\n",
      "|    total_timesteps    | 43000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -3.48    |\n",
      "|    explained_variance | 5.96e-08 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 8599     |\n",
      "|    policy_loss        | -0.0044  |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 7.84     |\n",
      "|    value_loss         | 2.13e-06 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 232      |\n",
      "|    iterations         | 8700     |\n",
      "|    time_elapsed       | 186      |\n",
      "|    total_timesteps    | 43500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -3.5     |\n",
      "|    explained_variance | 5.96e-08 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 8699     |\n",
      "|    policy_loss        | -0.0392  |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 8.04     |\n",
      "|    value_loss         | 0.000123 |\n",
      "------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 232         |\n",
      "|    iterations         | 8800        |\n",
      "|    time_elapsed       | 189         |\n",
      "|    total_timesteps    | 44000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -3.52       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 8799        |\n",
      "|    policy_loss        | -0.104      |\n",
      "|    reward             | 0.030748487 |\n",
      "|    std                | 8.19        |\n",
      "|    value_loss         | 0.000672    |\n",
      "---------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 232      |\n",
      "|    iterations         | 8900     |\n",
      "|    time_elapsed       | 191      |\n",
      "|    total_timesteps    | 44500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -3.55    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 8899     |\n",
      "|    policy_loss        | 0.0107   |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 8.41     |\n",
      "|    value_loss         | 7.9e-06  |\n",
      "------------------------------------\n",
      "day: 754, episode: 60\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 999019.57\n",
      "total_reward: -980.43\n",
      "total_cost: 8450.85\n",
      "total_trades: 404\n",
      "Sharpe: -0.049\n",
      "=================================\n",
      "-----------------------------------------\n",
      "| time/                 |               |\n",
      "|    fps                | 231           |\n",
      "|    iterations         | 9000          |\n",
      "|    time_elapsed       | 193           |\n",
      "|    total_timesteps    | 45000         |\n",
      "| train/                |               |\n",
      "|    entropy_loss       | -3.56         |\n",
      "|    explained_variance | 0             |\n",
      "|    learning_rate      | 0.0007        |\n",
      "|    n_updates          | 8999          |\n",
      "|    policy_loss        | -0.0455       |\n",
      "|    reward             | -0.0031865656 |\n",
      "|    std                | 8.53          |\n",
      "|    value_loss         | 0.00201       |\n",
      "-----------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 231      |\n",
      "|    iterations         | 9100     |\n",
      "|    time_elapsed       | 196      |\n",
      "|    total_timesteps    | 45500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -3.58    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 9099     |\n",
      "|    policy_loss        | -0.0166  |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 8.68     |\n",
      "|    value_loss         | 2.5e-05  |\n",
      "------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                 |               |\n",
      "|    fps                | 231           |\n",
      "|    iterations         | 9200          |\n",
      "|    time_elapsed       | 198           |\n",
      "|    total_timesteps    | 46000         |\n",
      "| train/                |               |\n",
      "|    entropy_loss       | -3.59         |\n",
      "|    explained_variance | 0             |\n",
      "|    learning_rate      | 0.0007        |\n",
      "|    n_updates          | 9199          |\n",
      "|    policy_loss        | -1.24         |\n",
      "|    reward             | -0.0015432639 |\n",
      "|    std                | 8.74          |\n",
      "|    value_loss         | 0.116         |\n",
      "-----------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 231         |\n",
      "|    iterations         | 9300        |\n",
      "|    time_elapsed       | 201         |\n",
      "|    total_timesteps    | 46500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -3.61       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 9299        |\n",
      "|    policy_loss        | -0.0262     |\n",
      "|    reward             | 0.012128938 |\n",
      "|    std                | 8.95        |\n",
      "|    value_loss         | 6.46e-05    |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 230         |\n",
      "|    iterations         | 9400        |\n",
      "|    time_elapsed       | 203         |\n",
      "|    total_timesteps    | 47000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -3.63       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 9399        |\n",
      "|    policy_loss        | 0.00308     |\n",
      "|    reward             | 0.022744471 |\n",
      "|    std                | 9.12        |\n",
      "|    value_loss         | 1.78e-05    |\n",
      "---------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 229      |\n",
      "|    iterations         | 9500     |\n",
      "|    time_elapsed       | 206      |\n",
      "|    total_timesteps    | 47500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -3.64    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 9499     |\n",
      "|    policy_loss        | 0.0123   |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 9.26     |\n",
      "|    value_loss         | 0.000754 |\n",
      "------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 228         |\n",
      "|    iterations         | 9600        |\n",
      "|    time_elapsed       | 209         |\n",
      "|    total_timesteps    | 48000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -3.66       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 9599        |\n",
      "|    policy_loss        | 0.108       |\n",
      "|    reward             | 0.036687467 |\n",
      "|    std                | 9.42        |\n",
      "|    value_loss         | 0.00129     |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 227         |\n",
      "|    iterations         | 9700        |\n",
      "|    time_elapsed       | 212         |\n",
      "|    total_timesteps    | 48500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -3.68       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 9699        |\n",
      "|    policy_loss        | 0.00931     |\n",
      "|    reward             | -0.04108589 |\n",
      "|    std                | 9.56        |\n",
      "|    value_loss         | 0.000112    |\n",
      "---------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 227      |\n",
      "|    iterations         | 9800     |\n",
      "|    time_elapsed       | 215      |\n",
      "|    total_timesteps    | 49000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -3.69    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 9799     |\n",
      "|    policy_loss        | -0.335   |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 9.69     |\n",
      "|    value_loss         | 0.0112   |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 226       |\n",
      "|    iterations         | 9900      |\n",
      "|    time_elapsed       | 218       |\n",
      "|    total_timesteps    | 49500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -3.71     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 9899      |\n",
      "|    policy_loss        | 0.0119    |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 9.87      |\n",
      "|    value_loss         | 1.47e-05  |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 225        |\n",
      "|    iterations         | 10000      |\n",
      "|    time_elapsed       | 222        |\n",
      "|    total_timesteps    | 50000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -3.73      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 9999       |\n",
      "|    policy_loss        | 0.221      |\n",
      "|    reward             | 0.06588285 |\n",
      "|    std                | 10.1       |\n",
      "|    value_loss         | 0.00377    |\n",
      "--------------------------------------\n"
     ]
    }
   ],
   "source": [
    "trained_a2c = agent.train_model(model=model_a2c, \n",
    "                             tb_log_name='a2c',\n",
    "                             total_timesteps=50000) if if_using_a2c else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "zjCWfgsg3sVa"
   },
   "outputs": [],
   "source": [
    "trained_a2c.save(TRAINED_MODEL_DIR + \"/agent_a2c\") if if_using_a2c else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MRiOtrywfAo1"
   },
   "source": [
    "### Agent 2: DDPG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "M2YadjfnLwgt"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'batch_size': 128, 'buffer_size': 50000, 'learning_rate': 0.001}\n",
      "Using cuda device\n",
      "Logging to results/ddpg\n"
     ]
    }
   ],
   "source": [
    "agent = DRLAgent(env = env_train)\n",
    "model_ddpg = agent.get_model(\"ddpg\")\n",
    "\n",
    "if if_using_ddpg:\n",
    "  # set up logger\n",
    "  tmp_path = RESULTS_DIR + '/ddpg'\n",
    "  new_logger_ddpg = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n",
    "  # Set new logger\n",
    "  model_ddpg.set_logger(new_logger_ddpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "tCDa78rqfO_a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "day: 754, episode: 70\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1000000.00\n",
      "total_reward: 0.00\n",
      "total_cost: 0.00\n",
      "total_trades: 0\n",
      "=================================\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 4         |\n",
      "|    fps             | 111       |\n",
      "|    time_elapsed    | 27        |\n",
      "|    total_timesteps | 3020      |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -2.27e+03 |\n",
      "|    critic_loss     | 1.39e+04  |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 2919      |\n",
      "|    reward          | 0.0       |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 8         |\n",
      "|    fps             | 99        |\n",
      "|    time_elapsed    | 60        |\n",
      "|    total_timesteps | 6040      |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -4.63e+03 |\n",
      "|    critic_loss     | 140       |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 5939      |\n",
      "|    reward          | 0.0       |\n",
      "----------------------------------\n",
      "day: 754, episode: 80\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1000000.00\n",
      "total_reward: 0.00\n",
      "total_cost: 0.00\n",
      "total_trades: 0\n",
      "=================================\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 12        |\n",
      "|    fps             | 91        |\n",
      "|    time_elapsed    | 98        |\n",
      "|    total_timesteps | 9060      |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -4.06e+03 |\n",
      "|    critic_loss     | 946       |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 8959      |\n",
      "|    reward          | 0.0       |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 16        |\n",
      "|    fps             | 88        |\n",
      "|    time_elapsed    | 136       |\n",
      "|    total_timesteps | 12080     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -3.32e+03 |\n",
      "|    critic_loss     | 539       |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 11979     |\n",
      "|    reward          | 0.0       |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 20        |\n",
      "|    fps             | 93        |\n",
      "|    time_elapsed    | 162       |\n",
      "|    total_timesteps | 15100     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -3.14e+03 |\n",
      "|    critic_loss     | 2.31e+03  |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 14999     |\n",
      "|    reward          | 0.0       |\n",
      "----------------------------------\n",
      "day: 754, episode: 90\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1000000.00\n",
      "total_reward: 0.00\n",
      "total_cost: 0.00\n",
      "total_trades: 0\n",
      "=================================\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 24       |\n",
      "|    fps             | 88       |\n",
      "|    time_elapsed    | 204      |\n",
      "|    total_timesteps | 18120    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.6e+03 |\n",
      "|    critic_loss     | 801      |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 18019    |\n",
      "|    reward          | 0.0      |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 28        |\n",
      "|    fps             | 89        |\n",
      "|    time_elapsed    | 236       |\n",
      "|    total_timesteps | 21140     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -2.11e+03 |\n",
      "|    critic_loss     | 7.33e+04  |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 21039     |\n",
      "|    reward          | 0.0       |\n",
      "----------------------------------\n",
      "day: 754, episode: 100\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1000000.00\n",
      "total_reward: 0.00\n",
      "total_cost: 0.00\n",
      "total_trades: 0\n",
      "=================================\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 32        |\n",
      "|    fps             | 92        |\n",
      "|    time_elapsed    | 262       |\n",
      "|    total_timesteps | 24160     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.95e+03 |\n",
      "|    critic_loss     | 124       |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 24059     |\n",
      "|    reward          | 0.0       |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 36        |\n",
      "|    fps             | 93        |\n",
      "|    time_elapsed    | 291       |\n",
      "|    total_timesteps | 27180     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.63e+03 |\n",
      "|    critic_loss     | 757       |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 27079     |\n",
      "|    reward          | 0.0       |\n",
      "----------------------------------\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 40       |\n",
      "|    fps             | 91       |\n",
      "|    time_elapsed    | 331      |\n",
      "|    total_timesteps | 30200    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.4e+03 |\n",
      "|    critic_loss     | 232      |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 30099    |\n",
      "|    reward          | 0.0      |\n",
      "---------------------------------\n",
      "day: 754, episode: 110\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1000000.00\n",
      "total_reward: 0.00\n",
      "total_cost: 0.00\n",
      "total_trades: 0\n",
      "=================================\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 44        |\n",
      "|    fps             | 89        |\n",
      "|    time_elapsed    | 371       |\n",
      "|    total_timesteps | 33220     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.16e+03 |\n",
      "|    critic_loss     | 2.49      |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 33119     |\n",
      "|    reward          | 0.0       |\n",
      "----------------------------------\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 48       |\n",
      "|    fps             | 91       |\n",
      "|    time_elapsed    | 396      |\n",
      "|    total_timesteps | 36240    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.0913  |\n",
      "|    critic_loss     | 4.61e-05 |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 36139    |\n",
      "|    reward          | 0.0      |\n",
      "---------------------------------\n",
      "day: 754, episode: 120\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1000000.00\n",
      "total_reward: 0.00\n",
      "total_cost: 0.00\n",
      "total_trades: 0\n",
      "=================================\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 52       |\n",
      "|    fps             | 92       |\n",
      "|    time_elapsed    | 425      |\n",
      "|    total_timesteps | 39260    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.0912  |\n",
      "|    critic_loss     | 8.15e-07 |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 39159    |\n",
      "|    reward          | 0.0      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 56       |\n",
      "|    fps             | 92       |\n",
      "|    time_elapsed    | 459      |\n",
      "|    total_timesteps | 42280    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.0908  |\n",
      "|    critic_loss     | 7.52e-07 |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 42179    |\n",
      "|    reward          | 0.0      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 60       |\n",
      "|    fps             | 92       |\n",
      "|    time_elapsed    | 492      |\n",
      "|    total_timesteps | 45300    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.0893  |\n",
      "|    critic_loss     | 5.29e-07 |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 45199    |\n",
      "|    reward          | 0.0      |\n",
      "---------------------------------\n",
      "day: 754, episode: 130\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1000000.00\n",
      "total_reward: 0.00\n",
      "total_cost: 0.00\n",
      "total_trades: 0\n",
      "=================================\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 64       |\n",
      "|    fps             | 93       |\n",
      "|    time_elapsed    | 516      |\n",
      "|    total_timesteps | 48320    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.0845  |\n",
      "|    critic_loss     | 0.000112 |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 48219    |\n",
      "|    reward          | 0.0      |\n",
      "---------------------------------\n"
     ]
    }
   ],
   "source": [
    "trained_ddpg = agent.train_model(model=model_ddpg, \n",
    "                             tb_log_name='ddpg',\n",
    "                             total_timesteps=50000) if if_using_ddpg else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "ne6M2R-WvrUQ"
   },
   "outputs": [],
   "source": [
    "trained_ddpg.save(TRAINED_MODEL_DIR + \"/agent_ddpg\") if if_using_ddpg else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_gDkU-j-fCmZ"
   },
   "source": [
    "### Agent 3: PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "y5D5PFUhMzSV"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_steps': 2048, 'ent_coef': 0.01, 'learning_rate': 0.00025, 'batch_size': 128}\n",
      "Using cuda device\n",
      "Logging to results/ppo\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Development\\Anaconda3\\envs\\trading\\lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:150: UserWarning: You are trying to run PPO on the GPU, but it is primarily intended to run on the CPU when not using a CNN policy (you are using ActorCriticPolicy which should be a MlpPolicy). See https://github.com/DLR-RM/stable-baselines3/issues/1245 for more info. You can pass `device='cpu'` or `export CUDA_VISIBLE_DEVICES=` to force using the CPU.Note: The model will train, but the GPU utilization will be poor and the training might take longer than on CPU.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "agent = DRLAgent(env = env_train)\n",
    "PPO_PARAMS = {\n",
    "    \"n_steps\": 2048,\n",
    "    \"ent_coef\": 0.01,\n",
    "    \"learning_rate\": 0.00025,\n",
    "    \"batch_size\": 128,\n",
    "}\n",
    "model_ppo = agent.get_model(\"ppo\",model_kwargs = PPO_PARAMS)\n",
    "\n",
    "if if_using_ppo:\n",
    "  # set up logger\n",
    "  tmp_path = RESULTS_DIR + '/ppo'\n",
    "  new_logger_ppo = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n",
    "  # Set new logger\n",
    "  model_ppo.set_logger(new_logger_ppo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "Gt8eIQKYM4G3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------\n",
      "| time/              |             |\n",
      "|    fps             | 251         |\n",
      "|    iterations      | 1           |\n",
      "|    time_elapsed    | 8           |\n",
      "|    total_timesteps | 2048        |\n",
      "| train/             |             |\n",
      "|    reward          | -0.46980911 |\n",
      "------------------------------------\n",
      "day: 754, episode: 140\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1091490.27\n",
      "total_reward: 91490.27\n",
      "total_cost: 10938.16\n",
      "total_trades: 746\n",
      "Sharpe: 0.285\n",
      "=================================\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 217          |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 18           |\n",
      "|    total_timesteps      | 4096         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0023698267 |\n",
      "|    clip_fraction        | 0.0142       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.42        |\n",
      "|    explained_variance   | 1.19e-07     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 0.73         |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | -0.000889    |\n",
      "|    reward               | -1.1561133   |\n",
      "|    std                  | 1.01         |\n",
      "|    value_loss           | 1.63         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 209         |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 29          |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005979374 |\n",
      "|    clip_fraction        | 0.0623      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.42       |\n",
      "|    explained_variance   | 0.0048      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 3.49        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.00543    |\n",
      "|    reward               | -0.62977004 |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 7.47        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 204          |\n",
      "|    iterations           | 4            |\n",
      "|    time_elapsed         | 39           |\n",
      "|    total_timesteps      | 8192         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0009904158 |\n",
      "|    clip_fraction        | 0.00176      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.42        |\n",
      "|    explained_variance   | -0.0348      |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 14.2         |\n",
      "|    n_updates            | 30           |\n",
      "|    policy_gradient_loss | -0.000468    |\n",
      "|    reward               | 3.648157     |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 24.9         |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 231           |\n",
      "|    iterations           | 5             |\n",
      "|    time_elapsed         | 44            |\n",
      "|    total_timesteps      | 10240         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00058088754 |\n",
      "|    clip_fraction        | 0.00488       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.42         |\n",
      "|    explained_variance   | 0.00908       |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 14.4          |\n",
      "|    n_updates            | 40            |\n",
      "|    policy_gradient_loss | 7.7e-05       |\n",
      "|    reward               | -0.76094306   |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 26.1          |\n",
      "-------------------------------------------\n",
      "day: 754, episode: 150\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1241198.90\n",
      "total_reward: 241198.90\n",
      "total_cost: 10172.65\n",
      "total_trades: 745\n",
      "Sharpe: 0.440\n",
      "=================================\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 253          |\n",
      "|    iterations           | 6            |\n",
      "|    time_elapsed         | 48           |\n",
      "|    total_timesteps      | 12288        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0034214742 |\n",
      "|    clip_fraction        | 0.0136       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.42        |\n",
      "|    explained_variance   | 0.00887      |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 7.02         |\n",
      "|    n_updates            | 50           |\n",
      "|    policy_gradient_loss | -0.00135     |\n",
      "|    reward               | -0.3907632   |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 17.6         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 260         |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 54          |\n",
      "|    total_timesteps      | 14336       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002656898 |\n",
      "|    clip_fraction        | 0.002       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.42       |\n",
      "|    explained_variance   | 0.0411      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 12          |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.000753   |\n",
      "|    reward               | 0.65588707  |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 21.6        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 250          |\n",
      "|    iterations           | 8            |\n",
      "|    time_elapsed         | 65           |\n",
      "|    total_timesteps      | 16384        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0035988213 |\n",
      "|    clip_fraction        | 0.0212       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.42        |\n",
      "|    explained_variance   | 0.0469       |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 6.98         |\n",
      "|    n_updates            | 70           |\n",
      "|    policy_gradient_loss | -0.003       |\n",
      "|    reward               | -2.286107    |\n",
      "|    std                  | 0.996        |\n",
      "|    value_loss           | 12.9         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 245          |\n",
      "|    iterations           | 9            |\n",
      "|    time_elapsed         | 75           |\n",
      "|    total_timesteps      | 18432        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0031762496 |\n",
      "|    clip_fraction        | 0.0168       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.41        |\n",
      "|    explained_variance   | 0.0725       |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 8.5          |\n",
      "|    n_updates            | 80           |\n",
      "|    policy_gradient_loss | 5.73e-05     |\n",
      "|    reward               | 0.2947212    |\n",
      "|    std                  | 0.991        |\n",
      "|    value_loss           | 15.3         |\n",
      "------------------------------------------\n",
      "day: 754, episode: 160\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1278217.27\n",
      "total_reward: 278217.27\n",
      "total_cost: 10918.11\n",
      "total_trades: 749\n",
      "Sharpe: 0.503\n",
      "=================================\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 244         |\n",
      "|    iterations           | 10          |\n",
      "|    time_elapsed         | 83          |\n",
      "|    total_timesteps      | 20480       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003327832 |\n",
      "|    clip_fraction        | 0.0253      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.41       |\n",
      "|    explained_variance   | 0.0789      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 13.9        |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.00173    |\n",
      "|    reward               | -1.5582802  |\n",
      "|    std                  | 0.992       |\n",
      "|    value_loss           | 25.2        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 244          |\n",
      "|    iterations           | 11           |\n",
      "|    time_elapsed         | 92           |\n",
      "|    total_timesteps      | 22528        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0016202658 |\n",
      "|    clip_fraction        | 0.0185       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.41        |\n",
      "|    explained_variance   | 0.0361       |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 10.4         |\n",
      "|    n_updates            | 100          |\n",
      "|    policy_gradient_loss | 0.00121      |\n",
      "|    reward               | -5.2278495   |\n",
      "|    std                  | 0.992        |\n",
      "|    value_loss           | 22.9         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 252          |\n",
      "|    iterations           | 12           |\n",
      "|    time_elapsed         | 97           |\n",
      "|    total_timesteps      | 24576        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.003949414  |\n",
      "|    clip_fraction        | 0.0383       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.41        |\n",
      "|    explained_variance   | 0.053        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 11.7         |\n",
      "|    n_updates            | 110          |\n",
      "|    policy_gradient_loss | -0.00252     |\n",
      "|    reward               | -0.031242386 |\n",
      "|    std                  | 0.986        |\n",
      "|    value_loss           | 20.8         |\n",
      "------------------------------------------\n",
      "day: 754, episode: 170\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1021431.45\n",
      "total_reward: 21431.45\n",
      "total_cost: 10653.96\n",
      "total_trades: 744\n",
      "Sharpe: 0.120\n",
      "=================================\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 262          |\n",
      "|    iterations           | 13           |\n",
      "|    time_elapsed         | 101          |\n",
      "|    total_timesteps      | 26624        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0015254891 |\n",
      "|    clip_fraction        | 0.0115       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.4         |\n",
      "|    explained_variance   | 0.0629       |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 2.49         |\n",
      "|    n_updates            | 120          |\n",
      "|    policy_gradient_loss | -0.000503    |\n",
      "|    reward               | 1.1120667    |\n",
      "|    std                  | 0.973        |\n",
      "|    value_loss           | 8.45         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 266          |\n",
      "|    iterations           | 14           |\n",
      "|    time_elapsed         | 107          |\n",
      "|    total_timesteps      | 28672        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0018105303 |\n",
      "|    clip_fraction        | 0.0301       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.39        |\n",
      "|    explained_variance   | 0.0561       |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 4.93         |\n",
      "|    n_updates            | 130          |\n",
      "|    policy_gradient_loss | -0.0029      |\n",
      "|    reward               | 2.5814514    |\n",
      "|    std                  | 0.978        |\n",
      "|    value_loss           | 10.1         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 261          |\n",
      "|    iterations           | 15           |\n",
      "|    time_elapsed         | 117          |\n",
      "|    total_timesteps      | 30720        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0034748851 |\n",
      "|    clip_fraction        | 0.0434       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.4         |\n",
      "|    explained_variance   | 0.0483       |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 9.75         |\n",
      "|    n_updates            | 140          |\n",
      "|    policy_gradient_loss | -0.00223     |\n",
      "|    reward               | 0.42787334   |\n",
      "|    std                  | 0.983        |\n",
      "|    value_loss           | 18.9         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 259          |\n",
      "|    iterations           | 16           |\n",
      "|    time_elapsed         | 126          |\n",
      "|    total_timesteps      | 32768        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0028324262 |\n",
      "|    clip_fraction        | 0.0179       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.41        |\n",
      "|    explained_variance   | 0.00551      |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 3.2          |\n",
      "|    n_updates            | 150          |\n",
      "|    policy_gradient_loss | -0.000475    |\n",
      "|    reward               | 0.0021061215 |\n",
      "|    std                  | 0.989        |\n",
      "|    value_loss           | 5.79         |\n",
      "------------------------------------------\n",
      "day: 754, episode: 180\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1043184.11\n",
      "total_reward: 43184.11\n",
      "total_cost: 10932.93\n",
      "total_trades: 745\n",
      "Sharpe: 0.187\n",
      "=================================\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 257          |\n",
      "|    iterations           | 17           |\n",
      "|    time_elapsed         | 135          |\n",
      "|    total_timesteps      | 34816        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0006618197 |\n",
      "|    clip_fraction        | 0.00889      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.42        |\n",
      "|    explained_variance   | -0.0487      |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 2.78         |\n",
      "|    n_updates            | 160          |\n",
      "|    policy_gradient_loss | -0.000201    |\n",
      "|    reward               | 0.21734108   |\n",
      "|    std                  | 1.01         |\n",
      "|    value_loss           | 6.36         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 256          |\n",
      "|    iterations           | 18           |\n",
      "|    time_elapsed         | 143          |\n",
      "|    total_timesteps      | 36864        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0009850176 |\n",
      "|    clip_fraction        | 0.0131       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.43        |\n",
      "|    explained_variance   | 0.00736      |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 2.5          |\n",
      "|    n_updates            | 170          |\n",
      "|    policy_gradient_loss | 0.00151      |\n",
      "|    reward               | 1.276409     |\n",
      "|    std                  | 1.01         |\n",
      "|    value_loss           | 5.67         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 254         |\n",
      "|    iterations           | 19          |\n",
      "|    time_elapsed         | 153         |\n",
      "|    total_timesteps      | 38912       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008045598 |\n",
      "|    clip_fraction        | 0.0407      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.43       |\n",
      "|    explained_variance   | -0.00947    |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 6.67        |\n",
      "|    n_updates            | 180         |\n",
      "|    policy_gradient_loss | -0.00223    |\n",
      "|    reward               | 0.27643165  |\n",
      "|    std                  | 1.02        |\n",
      "|    value_loss           | 11.5        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 253         |\n",
      "|    iterations           | 20          |\n",
      "|    time_elapsed         | 161         |\n",
      "|    total_timesteps      | 40960       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003991083 |\n",
      "|    clip_fraction        | 0.0324      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.43       |\n",
      "|    explained_variance   | -0.00471    |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 8.82        |\n",
      "|    n_updates            | 190         |\n",
      "|    policy_gradient_loss | 0.000611    |\n",
      "|    reward               | 0.31227776  |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 18.4        |\n",
      "-----------------------------------------\n",
      "day: 754, episode: 190\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1204287.51\n",
      "total_reward: 204287.51\n",
      "total_cost: 11115.89\n",
      "total_trades: 744\n",
      "Sharpe: 0.409\n",
      "=================================\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 251          |\n",
      "|    iterations           | 21           |\n",
      "|    time_elapsed         | 170          |\n",
      "|    total_timesteps      | 43008        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0010893992 |\n",
      "|    clip_fraction        | 0.0175       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.43        |\n",
      "|    explained_variance   | 0.000556     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 6.28         |\n",
      "|    n_updates            | 200          |\n",
      "|    policy_gradient_loss | -0.000215    |\n",
      "|    reward               | -4.683031    |\n",
      "|    std                  | 1.01         |\n",
      "|    value_loss           | 14.6         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 250          |\n",
      "|    iterations           | 22           |\n",
      "|    time_elapsed         | 180          |\n",
      "|    total_timesteps      | 45056        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0034151953 |\n",
      "|    clip_fraction        | 0.0205       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.43        |\n",
      "|    explained_variance   | 0.0224       |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 5.36         |\n",
      "|    n_updates            | 210          |\n",
      "|    policy_gradient_loss | -0.000176    |\n",
      "|    reward               | 0.52880996   |\n",
      "|    std                  | 1.01         |\n",
      "|    value_loss           | 15.5         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 243         |\n",
      "|    iterations           | 23          |\n",
      "|    time_elapsed         | 193         |\n",
      "|    total_timesteps      | 47104       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005174396 |\n",
      "|    clip_fraction        | 0.0372      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.43       |\n",
      "|    explained_variance   | 0.06        |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 6.78        |\n",
      "|    n_updates            | 220         |\n",
      "|    policy_gradient_loss | -0.000811   |\n",
      "|    reward               | 0.010110124 |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 17.5        |\n",
      "-----------------------------------------\n",
      "day: 754, episode: 200\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1069214.04\n",
      "total_reward: 69214.04\n",
      "total_cost: 10779.33\n",
      "total_trades: 744\n",
      "Sharpe: 0.260\n",
      "=================================\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 243           |\n",
      "|    iterations           | 24            |\n",
      "|    time_elapsed         | 202           |\n",
      "|    total_timesteps      | 49152         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00097503845 |\n",
      "|    clip_fraction        | 0.0337        |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.43         |\n",
      "|    explained_variance   | -0.00415      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 1.38          |\n",
      "|    n_updates            | 230           |\n",
      "|    policy_gradient_loss | -0.00169      |\n",
      "|    reward               | 0.2919647     |\n",
      "|    std                  | 1.01          |\n",
      "|    value_loss           | 4.27          |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 243          |\n",
      "|    iterations           | 25           |\n",
      "|    time_elapsed         | 210          |\n",
      "|    total_timesteps      | 51200        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0033893224 |\n",
      "|    clip_fraction        | 0.0196       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.43        |\n",
      "|    explained_variance   | -1.19e-07    |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 1.73         |\n",
      "|    n_updates            | 240          |\n",
      "|    policy_gradient_loss | -0.000867    |\n",
      "|    reward               | 1.877969     |\n",
      "|    std                  | 1.01         |\n",
      "|    value_loss           | 3.35         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 243         |\n",
      "|    iterations           | 26          |\n",
      "|    time_elapsed         | 218         |\n",
      "|    total_timesteps      | 53248       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.001489189 |\n",
      "|    clip_fraction        | 0.0279      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.43       |\n",
      "|    explained_variance   | -0.0408     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 1.69        |\n",
      "|    n_updates            | 250         |\n",
      "|    policy_gradient_loss | -0.000382   |\n",
      "|    reward               | 0.1107198   |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 2.78        |\n",
      "-----------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 243           |\n",
      "|    iterations           | 27            |\n",
      "|    time_elapsed         | 226           |\n",
      "|    total_timesteps      | 55296         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00071752665 |\n",
      "|    clip_fraction        | 0.0438        |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.43         |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 0.298         |\n",
      "|    n_updates            | 260           |\n",
      "|    policy_gradient_loss | -0.00161      |\n",
      "|    reward               | 0.16813636    |\n",
      "|    std                  | 1.01          |\n",
      "|    value_loss           | 1.21          |\n",
      "-------------------------------------------\n",
      "day: 754, episode: 210\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1011358.91\n",
      "total_reward: 11358.91\n",
      "total_cost: 9771.28\n",
      "total_trades: 706\n",
      "Sharpe: 0.200\n",
      "=================================\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 243          |\n",
      "|    iterations           | 28           |\n",
      "|    time_elapsed         | 235          |\n",
      "|    total_timesteps      | 57344        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0012529003 |\n",
      "|    clip_fraction        | 0.0158       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.42        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 0.0883       |\n",
      "|    n_updates            | 270          |\n",
      "|    policy_gradient_loss | -0.000599    |\n",
      "|    reward               | 0.007069705  |\n",
      "|    std                  | 0.99         |\n",
      "|    value_loss           | 0.207        |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 244         |\n",
      "|    iterations           | 29          |\n",
      "|    time_elapsed         | 242         |\n",
      "|    total_timesteps      | 59392       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004656286 |\n",
      "|    clip_fraction        | 0.0296      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.41       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 0.0577      |\n",
      "|    n_updates            | 280         |\n",
      "|    policy_gradient_loss | -0.00142    |\n",
      "|    reward               | -0.3791799  |\n",
      "|    std                  | 0.988       |\n",
      "|    value_loss           | 0.131       |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 247          |\n",
      "|    iterations           | 30           |\n",
      "|    time_elapsed         | 248          |\n",
      "|    total_timesteps      | 61440        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0021591636 |\n",
      "|    clip_fraction        | 0.0119       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.4         |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 0.928        |\n",
      "|    n_updates            | 290          |\n",
      "|    policy_gradient_loss | -0.000879    |\n",
      "|    reward               | -0.29756582  |\n",
      "|    std                  | 0.983        |\n",
      "|    value_loss           | 2.45         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 249          |\n",
      "|    iterations           | 31           |\n",
      "|    time_elapsed         | 253          |\n",
      "|    total_timesteps      | 63488        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.003801424  |\n",
      "|    clip_fraction        | 0.0116       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.4         |\n",
      "|    explained_variance   | 0.00656      |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 4.46         |\n",
      "|    n_updates            | 300          |\n",
      "|    policy_gradient_loss | -0.000278    |\n",
      "|    reward               | -0.035885226 |\n",
      "|    std                  | 0.983        |\n",
      "|    value_loss           | 8.94         |\n",
      "------------------------------------------\n",
      "day: 754, episode: 220\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1042797.72\n",
      "total_reward: 42797.72\n",
      "total_cost: 9388.69\n",
      "total_trades: 684\n",
      "Sharpe: 0.334\n",
      "=================================\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 251          |\n",
      "|    iterations           | 32           |\n",
      "|    time_elapsed         | 260          |\n",
      "|    total_timesteps      | 65536        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0035364288 |\n",
      "|    clip_fraction        | 0.0308       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.4         |\n",
      "|    explained_variance   | 5.96e-08     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 0.365        |\n",
      "|    n_updates            | 310          |\n",
      "|    policy_gradient_loss | -0.00179     |\n",
      "|    reward               | 0.0          |\n",
      "|    std                  | 0.988        |\n",
      "|    value_loss           | 0.853        |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 252           |\n",
      "|    iterations           | 33            |\n",
      "|    time_elapsed         | 267           |\n",
      "|    total_timesteps      | 67584         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00073635124 |\n",
      "|    clip_fraction        | 9.77e-05      |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.41         |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 0.284         |\n",
      "|    n_updates            | 320           |\n",
      "|    policy_gradient_loss | 0.000218      |\n",
      "|    reward               | 0.005247623   |\n",
      "|    std                  | 0.995         |\n",
      "|    value_loss           | 0.45          |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 252          |\n",
      "|    iterations           | 34           |\n",
      "|    time_elapsed         | 275          |\n",
      "|    total_timesteps      | 69632        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0052342094 |\n",
      "|    clip_fraction        | 0.0266       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.41        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 0.116        |\n",
      "|    n_updates            | 330          |\n",
      "|    policy_gradient_loss | -0.00162     |\n",
      "|    reward               | 0.27213058   |\n",
      "|    std                  | 0.988        |\n",
      "|    value_loss           | 0.236        |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 251          |\n",
      "|    iterations           | 35           |\n",
      "|    time_elapsed         | 284          |\n",
      "|    total_timesteps      | 71680        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0038001249 |\n",
      "|    clip_fraction        | 0.0237       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.41        |\n",
      "|    explained_variance   | 0.000229     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 4.99         |\n",
      "|    n_updates            | 340          |\n",
      "|    policy_gradient_loss | -0.000774    |\n",
      "|    reward               | -0.31314957  |\n",
      "|    std                  | 0.985        |\n",
      "|    value_loss           | 12.6         |\n",
      "------------------------------------------\n",
      "day: 754, episode: 230\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1020791.37\n",
      "total_reward: 20791.37\n",
      "total_cost: 10480.75\n",
      "total_trades: 738\n",
      "Sharpe: 0.177\n",
      "=================================\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 252         |\n",
      "|    iterations           | 36          |\n",
      "|    time_elapsed         | 292         |\n",
      "|    total_timesteps      | 73728       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.001014441 |\n",
      "|    clip_fraction        | 0.000293    |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.41       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 0.359       |\n",
      "|    n_updates            | 350         |\n",
      "|    policy_gradient_loss | 0.000114    |\n",
      "|    reward               | 0.060568262 |\n",
      "|    std                  | 0.987       |\n",
      "|    value_loss           | 1.09        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 252          |\n",
      "|    iterations           | 37           |\n",
      "|    time_elapsed         | 300          |\n",
      "|    total_timesteps      | 75776        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0034596447 |\n",
      "|    clip_fraction        | 0.0064       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.41        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 1.02         |\n",
      "|    n_updates            | 360          |\n",
      "|    policy_gradient_loss | -0.00032     |\n",
      "|    reward               | -0.43380275  |\n",
      "|    std                  | 0.996        |\n",
      "|    value_loss           | 1.98         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 251          |\n",
      "|    iterations           | 38           |\n",
      "|    time_elapsed         | 309          |\n",
      "|    total_timesteps      | 77824        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0014303953 |\n",
      "|    clip_fraction        | 0.00259      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.42        |\n",
      "|    explained_variance   | 0.0339       |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 2.7          |\n",
      "|    n_updates            | 370          |\n",
      "|    policy_gradient_loss | 0.000165     |\n",
      "|    reward               | 0.0041845003 |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 5.18         |\n",
      "------------------------------------------\n",
      "day: 754, episode: 240\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1022842.44\n",
      "total_reward: 22842.44\n",
      "total_cost: 10992.04\n",
      "total_trades: 740\n",
      "Sharpe: 0.125\n",
      "=================================\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 249         |\n",
      "|    iterations           | 39          |\n",
      "|    time_elapsed         | 320         |\n",
      "|    total_timesteps      | 79872       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.001340531 |\n",
      "|    clip_fraction        | 0.0145      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.42       |\n",
      "|    explained_variance   | 0.0637      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 5.47        |\n",
      "|    n_updates            | 380         |\n",
      "|    policy_gradient_loss | -0.00171    |\n",
      "|    reward               | -0.3105543  |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 11.7        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 248          |\n",
      "|    iterations           | 40           |\n",
      "|    time_elapsed         | 330          |\n",
      "|    total_timesteps      | 81920        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0034816032 |\n",
      "|    clip_fraction        | 0.00322      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.42        |\n",
      "|    explained_variance   | 0.0731       |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 1.9          |\n",
      "|    n_updates            | 390          |\n",
      "|    policy_gradient_loss | -0.00109     |\n",
      "|    reward               | 0.003273035  |\n",
      "|    std                  | 1.01         |\n",
      "|    value_loss           | 4.33         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 247          |\n",
      "|    iterations           | 41           |\n",
      "|    time_elapsed         | 339          |\n",
      "|    total_timesteps      | 83968        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0032958842 |\n",
      "|    clip_fraction        | 0.00571      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.43        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 0.223        |\n",
      "|    n_updates            | 400          |\n",
      "|    policy_gradient_loss | 1.24e-05     |\n",
      "|    reward               | -0.029473132 |\n",
      "|    std                  | 1.01         |\n",
      "|    value_loss           | 0.51         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 247          |\n",
      "|    iterations           | 42           |\n",
      "|    time_elapsed         | 348          |\n",
      "|    total_timesteps      | 86016        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0002857495 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.43        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 2.03         |\n",
      "|    n_updates            | 410          |\n",
      "|    policy_gradient_loss | 0.0002       |\n",
      "|    reward               | 0.7763126    |\n",
      "|    std                  | 1.01         |\n",
      "|    value_loss           | 4.39         |\n",
      "------------------------------------------\n",
      "day: 754, episode: 250\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1153891.29\n",
      "total_reward: 153891.29\n",
      "total_cost: 10618.85\n",
      "total_trades: 733\n",
      "Sharpe: 0.850\n",
      "=================================\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 247          |\n",
      "|    iterations           | 43           |\n",
      "|    time_elapsed         | 355          |\n",
      "|    total_timesteps      | 88064        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0024720724 |\n",
      "|    clip_fraction        | 0.0101       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.43        |\n",
      "|    explained_variance   | -0.000618    |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 3.2          |\n",
      "|    n_updates            | 420          |\n",
      "|    policy_gradient_loss | -0.0014      |\n",
      "|    reward               | 0.005826447  |\n",
      "|    std                  | 1.01         |\n",
      "|    value_loss           | 8.41         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 247          |\n",
      "|    iterations           | 44           |\n",
      "|    time_elapsed         | 363          |\n",
      "|    total_timesteps      | 90112        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 3.400186e-05 |\n",
      "|    clip_fraction        | 4.88e-05     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.43        |\n",
      "|    explained_variance   | 0.0166       |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 1.64         |\n",
      "|    n_updates            | 430          |\n",
      "|    policy_gradient_loss | 0.000209     |\n",
      "|    reward               | 0.1434482    |\n",
      "|    std                  | 1.02         |\n",
      "|    value_loss           | 3.46         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 248          |\n",
      "|    iterations           | 45           |\n",
      "|    time_elapsed         | 371          |\n",
      "|    total_timesteps      | 92160        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0027637782 |\n",
      "|    clip_fraction        | 0.0106       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.44        |\n",
      "|    explained_variance   | 0.00706      |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 1.24         |\n",
      "|    n_updates            | 440          |\n",
      "|    policy_gradient_loss | -0.000966    |\n",
      "|    reward               | -0.014182638 |\n",
      "|    std                  | 1.02         |\n",
      "|    value_loss           | 3.54         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 248          |\n",
      "|    iterations           | 46           |\n",
      "|    time_elapsed         | 378          |\n",
      "|    total_timesteps      | 94208        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0059660547 |\n",
      "|    clip_fraction        | 0.0371       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.44        |\n",
      "|    explained_variance   | 0.0862       |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 4.96         |\n",
      "|    n_updates            | 450          |\n",
      "|    policy_gradient_loss | -0.00165     |\n",
      "|    reward               | 2.0904288    |\n",
      "|    std                  | 1.02         |\n",
      "|    value_loss           | 10.5         |\n",
      "------------------------------------------\n",
      "day: 754, episode: 260\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1318452.21\n",
      "total_reward: 318452.21\n",
      "total_cost: 10698.37\n",
      "total_trades: 747\n",
      "Sharpe: 0.542\n",
      "=================================\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 247          |\n",
      "|    iterations           | 47           |\n",
      "|    time_elapsed         | 388          |\n",
      "|    total_timesteps      | 96256        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0024048164 |\n",
      "|    clip_fraction        | 0.0351       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.44        |\n",
      "|    explained_variance   | -0.00329     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 5.7          |\n",
      "|    n_updates            | 460          |\n",
      "|    policy_gradient_loss | 0.00109      |\n",
      "|    reward               | -0.6070073   |\n",
      "|    std                  | 1.02         |\n",
      "|    value_loss           | 14.4         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 248         |\n",
      "|    iterations           | 48          |\n",
      "|    time_elapsed         | 395         |\n",
      "|    total_timesteps      | 98304       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008289175 |\n",
      "|    clip_fraction        | 0.0467      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.44       |\n",
      "|    explained_variance   | 0.0195      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 12.1        |\n",
      "|    n_updates            | 470         |\n",
      "|    policy_gradient_loss | 0.00168     |\n",
      "|    reward               | 0.26780263  |\n",
      "|    std                  | 1.02        |\n",
      "|    value_loss           | 21.4        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 249        |\n",
      "|    iterations           | 49         |\n",
      "|    time_elapsed         | 401        |\n",
      "|    total_timesteps      | 100352     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00542566 |\n",
      "|    clip_fraction        | 0.0262     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.43      |\n",
      "|    explained_variance   | 0.0069     |\n",
      "|    learning_rate        | 0.00025    |\n",
      "|    loss                 | 8.19       |\n",
      "|    n_updates            | 480        |\n",
      "|    policy_gradient_loss | -0.000399  |\n",
      "|    reward               | 0.94447714 |\n",
      "|    std                  | 1.01       |\n",
      "|    value_loss           | 15.1       |\n",
      "----------------------------------------\n",
      "day: 754, episode: 270\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1261456.93\n",
      "total_reward: 261456.93\n",
      "total_cost: 10770.45\n",
      "total_trades: 749\n",
      "Sharpe: 0.492\n",
      "=================================\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 250          |\n",
      "|    iterations           | 50           |\n",
      "|    time_elapsed         | 409          |\n",
      "|    total_timesteps      | 102400       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0036878306 |\n",
      "|    clip_fraction        | 0.0316       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.43        |\n",
      "|    explained_variance   | -0.00471     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 9.25         |\n",
      "|    n_updates            | 490          |\n",
      "|    policy_gradient_loss | 0.0006       |\n",
      "|    reward               | -0.24501045  |\n",
      "|    std                  | 1.01         |\n",
      "|    value_loss           | 18.4         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 250          |\n",
      "|    iterations           | 51           |\n",
      "|    time_elapsed         | 416          |\n",
      "|    total_timesteps      | 104448       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0051044244 |\n",
      "|    clip_fraction        | 0.021        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.43        |\n",
      "|    explained_variance   | 0.0355       |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 7.29         |\n",
      "|    n_updates            | 500          |\n",
      "|    policy_gradient_loss | -0.000318    |\n",
      "|    reward               | -0.19763823  |\n",
      "|    std                  | 1.01         |\n",
      "|    value_loss           | 13.3         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 250         |\n",
      "|    iterations           | 52          |\n",
      "|    time_elapsed         | 424         |\n",
      "|    total_timesteps      | 106496      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008782702 |\n",
      "|    clip_fraction        | 0.0324      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.43       |\n",
      "|    explained_variance   | 0.00978     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 6.66        |\n",
      "|    n_updates            | 510         |\n",
      "|    policy_gradient_loss | 0.00256     |\n",
      "|    reward               | 0.03044148  |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 13.5        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 250         |\n",
      "|    iterations           | 53          |\n",
      "|    time_elapsed         | 433         |\n",
      "|    total_timesteps      | 108544      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004105844 |\n",
      "|    clip_fraction        | 0.0219      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.42       |\n",
      "|    explained_variance   | 0.00046     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 4.99        |\n",
      "|    n_updates            | 520         |\n",
      "|    policy_gradient_loss | 0.000153    |\n",
      "|    reward               | 0.067821    |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 6.71        |\n",
      "-----------------------------------------\n",
      "day: 754, episode: 280\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1055725.25\n",
      "total_reward: 55725.25\n",
      "total_cost: 10617.63\n",
      "total_trades: 725\n",
      "Sharpe: 0.399\n",
      "=================================\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 249          |\n",
      "|    iterations           | 54           |\n",
      "|    time_elapsed         | 443          |\n",
      "|    total_timesteps      | 110592       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0054904083 |\n",
      "|    clip_fraction        | 0.0225       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.42        |\n",
      "|    explained_variance   | 0.0121       |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 4.96         |\n",
      "|    n_updates            | 530          |\n",
      "|    policy_gradient_loss | -0.000426    |\n",
      "|    reward               | 0.13308017   |\n",
      "|    std                  | 1.01         |\n",
      "|    value_loss           | 8.83         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 247          |\n",
      "|    iterations           | 55           |\n",
      "|    time_elapsed         | 454          |\n",
      "|    total_timesteps      | 112640       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0040617147 |\n",
      "|    clip_fraction        | 0.0336       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.43        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 0.484        |\n",
      "|    n_updates            | 540          |\n",
      "|    policy_gradient_loss | -0.0019      |\n",
      "|    reward               | 0.27303076   |\n",
      "|    std                  | 1.01         |\n",
      "|    value_loss           | 0.747        |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 246         |\n",
      "|    iterations           | 56          |\n",
      "|    time_elapsed         | 464         |\n",
      "|    total_timesteps      | 114688      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004458174 |\n",
      "|    clip_fraction        | 0.027       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.43       |\n",
      "|    explained_variance   | 0.000111    |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 5.75        |\n",
      "|    n_updates            | 550         |\n",
      "|    policy_gradient_loss | -0.00146    |\n",
      "|    reward               | 0.23869272  |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 9.57        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 245         |\n",
      "|    iterations           | 57          |\n",
      "|    time_elapsed         | 475         |\n",
      "|    total_timesteps      | 116736      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002004349 |\n",
      "|    clip_fraction        | 0.03        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.43       |\n",
      "|    explained_variance   | -1.19e-07   |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 1.37        |\n",
      "|    n_updates            | 560         |\n",
      "|    policy_gradient_loss | -0.00127    |\n",
      "|    reward               | 0.5362781   |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 2.02        |\n",
      "-----------------------------------------\n",
      "day: 754, episode: 290\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 988008.88\n",
      "total_reward: -11991.12\n",
      "total_cost: 10725.82\n",
      "total_trades: 726\n",
      "Sharpe: 0.036\n",
      "=================================\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 244          |\n",
      "|    iterations           | 58           |\n",
      "|    time_elapsed         | 486          |\n",
      "|    total_timesteps      | 118784       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0011838351 |\n",
      "|    clip_fraction        | 0.0234       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.43        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 0.792        |\n",
      "|    n_updates            | 570          |\n",
      "|    policy_gradient_loss | -0.00166     |\n",
      "|    reward               | 0.17929603   |\n",
      "|    std                  | 1.01         |\n",
      "|    value_loss           | 1.57         |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 243           |\n",
      "|    iterations           | 59            |\n",
      "|    time_elapsed         | 496           |\n",
      "|    total_timesteps      | 120832        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.0035202675  |\n",
      "|    clip_fraction        | 0.0229        |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.44         |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 1.33          |\n",
      "|    n_updates            | 580           |\n",
      "|    policy_gradient_loss | -0.00156      |\n",
      "|    reward               | -0.0017320607 |\n",
      "|    std                  | 1.02          |\n",
      "|    value_loss           | 3.32          |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 243          |\n",
      "|    iterations           | 60           |\n",
      "|    time_elapsed         | 504          |\n",
      "|    total_timesteps      | 122880       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0015878679 |\n",
      "|    clip_fraction        | 0.0191       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.44        |\n",
      "|    explained_variance   | 0.0102       |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 4.15         |\n",
      "|    n_updates            | 590          |\n",
      "|    policy_gradient_loss | -0.00179     |\n",
      "|    reward               | 0.091025025  |\n",
      "|    std                  | 1.02         |\n",
      "|    value_loss           | 8.85         |\n",
      "------------------------------------------\n",
      "day: 754, episode: 300\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1208263.24\n",
      "total_reward: 208263.24\n",
      "total_cost: 10855.46\n",
      "total_trades: 747\n",
      "Sharpe: 0.422\n",
      "=================================\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 245         |\n",
      "|    iterations           | 61          |\n",
      "|    time_elapsed         | 508         |\n",
      "|    total_timesteps      | 124928      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003195838 |\n",
      "|    clip_fraction        | 0.0159      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.44       |\n",
      "|    explained_variance   | 0.0108      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 6.37        |\n",
      "|    n_updates            | 600         |\n",
      "|    policy_gradient_loss | 0.00153     |\n",
      "|    reward               | -1.1533566  |\n",
      "|    std                  | 1.02        |\n",
      "|    value_loss           | 14.6        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 247          |\n",
      "|    iterations           | 62           |\n",
      "|    time_elapsed         | 512          |\n",
      "|    total_timesteps      | 126976       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0023203907 |\n",
      "|    clip_fraction        | 0.00835      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.44        |\n",
      "|    explained_variance   | 0.00381      |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 6.74         |\n",
      "|    n_updates            | 610          |\n",
      "|    policy_gradient_loss | -5.06e-05    |\n",
      "|    reward               | 0.55176634   |\n",
      "|    std                  | 1.02         |\n",
      "|    value_loss           | 15.2         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 247         |\n",
      "|    iterations           | 63          |\n",
      "|    time_elapsed         | 521         |\n",
      "|    total_timesteps      | 129024      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005406485 |\n",
      "|    clip_fraction        | 0.0432      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.43       |\n",
      "|    explained_variance   | 0.00199     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 2.58        |\n",
      "|    n_updates            | 620         |\n",
      "|    policy_gradient_loss | -0.00281    |\n",
      "|    reward               | -0.77898586 |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 3.3         |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 246          |\n",
      "|    iterations           | 64           |\n",
      "|    time_elapsed         | 531          |\n",
      "|    total_timesteps      | 131072       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0026547313 |\n",
      "|    clip_fraction        | 0.0184       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.44        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 0.128        |\n",
      "|    n_updates            | 630          |\n",
      "|    policy_gradient_loss | -0.000504    |\n",
      "|    reward               | -0.053033054 |\n",
      "|    std                  | 1.02         |\n",
      "|    value_loss           | 0.231        |\n",
      "------------------------------------------\n",
      "day: 754, episode: 310\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1034596.10\n",
      "total_reward: 34596.10\n",
      "total_cost: 9807.03\n",
      "total_trades: 703\n",
      "Sharpe: 0.340\n",
      "=================================\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 245          |\n",
      "|    iterations           | 65           |\n",
      "|    time_elapsed         | 542          |\n",
      "|    total_timesteps      | 133120       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0025582244 |\n",
      "|    clip_fraction        | 0.0215       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.43        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 0.152        |\n",
      "|    n_updates            | 640          |\n",
      "|    policy_gradient_loss | -0.000981    |\n",
      "|    reward               | -0.18030646  |\n",
      "|    std                  | 1.01         |\n",
      "|    value_loss           | 0.254        |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 244          |\n",
      "|    iterations           | 66           |\n",
      "|    time_elapsed         | 552          |\n",
      "|    total_timesteps      | 135168       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0024150205 |\n",
      "|    clip_fraction        | 0.00381      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.43        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 0.145        |\n",
      "|    n_updates            | 650          |\n",
      "|    policy_gradient_loss | 0.000115     |\n",
      "|    reward               | 0.0023216438 |\n",
      "|    std                  | 1.02         |\n",
      "|    value_loss           | 0.44         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 243          |\n",
      "|    iterations           | 67           |\n",
      "|    time_elapsed         | 562          |\n",
      "|    total_timesteps      | 137216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0016249905 |\n",
      "|    clip_fraction        | 0.00645      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.44        |\n",
      "|    explained_variance   | -0.00128     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 2.82         |\n",
      "|    n_updates            | 660          |\n",
      "|    policy_gradient_loss | -0.000768    |\n",
      "|    reward               | -0.035302114 |\n",
      "|    std                  | 1.02         |\n",
      "|    value_loss           | 5.84         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 243          |\n",
      "|    iterations           | 68           |\n",
      "|    time_elapsed         | 572          |\n",
      "|    total_timesteps      | 139264       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0020737033 |\n",
      "|    clip_fraction        | 0.00352      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.45        |\n",
      "|    explained_variance   | 1.19e-07     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 0.184        |\n",
      "|    n_updates            | 670          |\n",
      "|    policy_gradient_loss | -0.000203    |\n",
      "|    reward               | -0.01377172  |\n",
      "|    std                  | 1.04         |\n",
      "|    value_loss           | 0.564        |\n",
      "------------------------------------------\n",
      "day: 754, episode: 320\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1054385.13\n",
      "total_reward: 54385.13\n",
      "total_cost: 11039.81\n",
      "total_trades: 735\n",
      "Sharpe: 0.322\n",
      "=================================\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 242          |\n",
      "|    iterations           | 69           |\n",
      "|    time_elapsed         | 581          |\n",
      "|    total_timesteps      | 141312       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0014919867 |\n",
      "|    clip_fraction        | 0.000488     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.45        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 0.759        |\n",
      "|    n_updates            | 680          |\n",
      "|    policy_gradient_loss | 0.000116     |\n",
      "|    reward               | 0.1601739    |\n",
      "|    std                  | 1.03         |\n",
      "|    value_loss           | 1.42         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 243          |\n",
      "|    iterations           | 70           |\n",
      "|    time_elapsed         | 589          |\n",
      "|    total_timesteps      | 143360       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0017013624 |\n",
      "|    clip_fraction        | 0.00498      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.45        |\n",
      "|    explained_variance   | -0.000736    |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 5.6          |\n",
      "|    n_updates            | 690          |\n",
      "|    policy_gradient_loss | -0.000668    |\n",
      "|    reward               | 1.3776608    |\n",
      "|    std                  | 1.04         |\n",
      "|    value_loss           | 9.2          |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 244         |\n",
      "|    iterations           | 71          |\n",
      "|    time_elapsed         | 593         |\n",
      "|    total_timesteps      | 145408      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007156223 |\n",
      "|    clip_fraction        | 0.035       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.46       |\n",
      "|    explained_variance   | 0.0346      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 7.36        |\n",
      "|    n_updates            | 700         |\n",
      "|    policy_gradient_loss | -0.00371    |\n",
      "|    reward               | -0.48031956 |\n",
      "|    std                  | 1.04        |\n",
      "|    value_loss           | 14.4        |\n",
      "-----------------------------------------\n",
      "day: 754, episode: 330\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1057491.84\n",
      "total_reward: 57491.84\n",
      "total_cost: 10886.73\n",
      "total_trades: 743\n",
      "Sharpe: 0.202\n",
      "=================================\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 246          |\n",
      "|    iterations           | 72           |\n",
      "|    time_elapsed         | 597          |\n",
      "|    total_timesteps      | 147456       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0008631237 |\n",
      "|    clip_fraction        | 0.00215      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.46        |\n",
      "|    explained_variance   | -0.0331      |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 1.39         |\n",
      "|    n_updates            | 710          |\n",
      "|    policy_gradient_loss | 0.000635     |\n",
      "|    reward               | -0.004094985 |\n",
      "|    std                  | 1.04         |\n",
      "|    value_loss           | 1.71         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 247          |\n",
      "|    iterations           | 73           |\n",
      "|    time_elapsed         | 604          |\n",
      "|    total_timesteps      | 149504       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0067369295 |\n",
      "|    clip_fraction        | 0.0578       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.45        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 2.58         |\n",
      "|    n_updates            | 720          |\n",
      "|    policy_gradient_loss | -0.00496     |\n",
      "|    reward               | 0.012070775  |\n",
      "|    std                  | 1.03         |\n",
      "|    value_loss           | 4.88         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 247          |\n",
      "|    iterations           | 74           |\n",
      "|    time_elapsed         | 612          |\n",
      "|    total_timesteps      | 151552       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0040738923 |\n",
      "|    clip_fraction        | 0.0297       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.45        |\n",
      "|    explained_variance   | -1.19e-07    |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 0.385        |\n",
      "|    n_updates            | 730          |\n",
      "|    policy_gradient_loss | -0.00196     |\n",
      "|    reward               | -0.08914121  |\n",
      "|    std                  | 1.04         |\n",
      "|    value_loss           | 0.616        |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 247           |\n",
      "|    iterations           | 75            |\n",
      "|    time_elapsed         | 621           |\n",
      "|    total_timesteps      | 153600        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.0064126775  |\n",
      "|    clip_fraction        | 0.0741        |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.46         |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 0.00965       |\n",
      "|    n_updates            | 740           |\n",
      "|    policy_gradient_loss | -0.00567      |\n",
      "|    reward               | -0.0065263486 |\n",
      "|    std                  | 1.04          |\n",
      "|    value_loss           | 0.0443        |\n",
      "-------------------------------------------\n",
      "day: 754, episode: 340\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1048893.47\n",
      "total_reward: 48893.47\n",
      "total_cost: 10768.07\n",
      "total_trades: 737\n",
      "Sharpe: 0.181\n",
      "=================================\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 247         |\n",
      "|    iterations           | 76          |\n",
      "|    time_elapsed         | 628         |\n",
      "|    total_timesteps      | 155648      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002424753 |\n",
      "|    clip_fraction        | 0.0107      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.46       |\n",
      "|    explained_variance   | 1.19e-07    |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 0.14        |\n",
      "|    n_updates            | 750         |\n",
      "|    policy_gradient_loss | -0.000714   |\n",
      "|    reward               | 1.0959467   |\n",
      "|    std                  | 1.04        |\n",
      "|    value_loss           | 0.519       |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 247          |\n",
      "|    iterations           | 77           |\n",
      "|    time_elapsed         | 636          |\n",
      "|    total_timesteps      | 157696       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0020406442 |\n",
      "|    clip_fraction        | 0.0021       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.46        |\n",
      "|    explained_variance   | 0.00447      |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 2.6          |\n",
      "|    n_updates            | 760          |\n",
      "|    policy_gradient_loss | -0.00071     |\n",
      "|    reward               | 0.8498768    |\n",
      "|    std                  | 1.05         |\n",
      "|    value_loss           | 4.15         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 247         |\n",
      "|    iterations           | 78          |\n",
      "|    time_elapsed         | 644         |\n",
      "|    total_timesteps      | 159744      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005976442 |\n",
      "|    clip_fraction        | 0.0472      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.47       |\n",
      "|    explained_variance   | -0.0443     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 3.62        |\n",
      "|    n_updates            | 770         |\n",
      "|    policy_gradient_loss | -0.0044     |\n",
      "|    reward               | 0.002223058 |\n",
      "|    std                  | 1.05        |\n",
      "|    value_loss           | 9.17        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 247          |\n",
      "|    iterations           | 79           |\n",
      "|    time_elapsed         | 653          |\n",
      "|    total_timesteps      | 161792       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0012597297 |\n",
      "|    clip_fraction        | 0.00596      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.47        |\n",
      "|    explained_variance   | 0.00588      |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 9.52         |\n",
      "|    n_updates            | 780          |\n",
      "|    policy_gradient_loss | -0.000877    |\n",
      "|    reward               | 0.27315384   |\n",
      "|    std                  | 1.05         |\n",
      "|    value_loss           | 21.9         |\n",
      "------------------------------------------\n",
      "day: 754, episode: 350\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1342232.79\n",
      "total_reward: 342232.79\n",
      "total_cost: 10644.38\n",
      "total_trades: 750\n",
      "Sharpe: 0.536\n",
      "=================================\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 247           |\n",
      "|    iterations           | 80            |\n",
      "|    time_elapsed         | 661           |\n",
      "|    total_timesteps      | 163840        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00092813687 |\n",
      "|    clip_fraction        | 0.00791       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.47         |\n",
      "|    explained_variance   | 0.0046        |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 11.1          |\n",
      "|    n_updates            | 790           |\n",
      "|    policy_gradient_loss | -0.00157      |\n",
      "|    reward               | -0.002633558  |\n",
      "|    std                  | 1.06          |\n",
      "|    value_loss           | 26            |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 249          |\n",
      "|    iterations           | 81           |\n",
      "|    time_elapsed         | 666          |\n",
      "|    total_timesteps      | 165888       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0030165203 |\n",
      "|    clip_fraction        | 0.0196       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.47        |\n",
      "|    explained_variance   | 0.00141      |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 11.7         |\n",
      "|    n_updates            | 800          |\n",
      "|    policy_gradient_loss | -0.00238     |\n",
      "|    reward               | -0.24845722  |\n",
      "|    std                  | 1.05         |\n",
      "|    value_loss           | 25.2         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 250          |\n",
      "|    iterations           | 82           |\n",
      "|    time_elapsed         | 670          |\n",
      "|    total_timesteps      | 167936       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0042807064 |\n",
      "|    clip_fraction        | 0.0366       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.47        |\n",
      "|    explained_variance   | 0.00732      |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 7.38         |\n",
      "|    n_updates            | 810          |\n",
      "|    policy_gradient_loss | -0.00202     |\n",
      "|    reward               | 0.521396     |\n",
      "|    std                  | 1.05         |\n",
      "|    value_loss           | 21.5         |\n",
      "------------------------------------------\n",
      "day: 754, episode: 360\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1453460.95\n",
      "total_reward: 453460.95\n",
      "total_cost: 10029.32\n",
      "total_trades: 749\n",
      "Sharpe: 0.619\n",
      "=================================\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 252          |\n",
      "|    iterations           | 83           |\n",
      "|    time_elapsed         | 674          |\n",
      "|    total_timesteps      | 169984       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0035602236 |\n",
      "|    clip_fraction        | 0.00645      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.47        |\n",
      "|    explained_variance   | -0.0104      |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 12.8         |\n",
      "|    n_updates            | 820          |\n",
      "|    policy_gradient_loss | -0.00126     |\n",
      "|    reward               | 4.3314686    |\n",
      "|    std                  | 1.05         |\n",
      "|    value_loss           | 29.3         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 253          |\n",
      "|    iterations           | 84           |\n",
      "|    time_elapsed         | 678          |\n",
      "|    total_timesteps      | 172032       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0007941342 |\n",
      "|    clip_fraction        | 0.00186      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.47        |\n",
      "|    explained_variance   | -0.00808     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 14.2         |\n",
      "|    n_updates            | 830          |\n",
      "|    policy_gradient_loss | -0.000456    |\n",
      "|    reward               | 4.110191     |\n",
      "|    std                  | 1.05         |\n",
      "|    value_loss           | 35.4         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 253          |\n",
      "|    iterations           | 85           |\n",
      "|    time_elapsed         | 686          |\n",
      "|    total_timesteps      | 174080       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0014120577 |\n",
      "|    clip_fraction        | 0.00723      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.47        |\n",
      "|    explained_variance   | 0.011        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 15.2         |\n",
      "|    n_updates            | 840          |\n",
      "|    policy_gradient_loss | -0.000811    |\n",
      "|    reward               | 1.6713797    |\n",
      "|    std                  | 1.05         |\n",
      "|    value_loss           | 31.1         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 253          |\n",
      "|    iterations           | 86           |\n",
      "|    time_elapsed         | 695          |\n",
      "|    total_timesteps      | 176128       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0024495393 |\n",
      "|    clip_fraction        | 0.0181       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.47        |\n",
      "|    explained_variance   | 0.000793     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 12.8         |\n",
      "|    n_updates            | 850          |\n",
      "|    policy_gradient_loss | -0.000462    |\n",
      "|    reward               | -2.7770941   |\n",
      "|    std                  | 1.06         |\n",
      "|    value_loss           | 31           |\n",
      "------------------------------------------\n",
      "day: 754, episode: 370\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1455270.51\n",
      "total_reward: 455270.51\n",
      "total_cost: 10068.87\n",
      "total_trades: 743\n",
      "Sharpe: 0.622\n",
      "=================================\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 253          |\n",
      "|    iterations           | 87           |\n",
      "|    time_elapsed         | 703          |\n",
      "|    total_timesteps      | 178176       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0049732896 |\n",
      "|    clip_fraction        | 0.0573       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.48        |\n",
      "|    explained_variance   | 0.0019       |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 21.6         |\n",
      "|    n_updates            | 860          |\n",
      "|    policy_gradient_loss | -0.000483    |\n",
      "|    reward               | -2.1055903   |\n",
      "|    std                  | 1.06         |\n",
      "|    value_loss           | 35.4         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 252          |\n",
      "|    iterations           | 88           |\n",
      "|    time_elapsed         | 713          |\n",
      "|    total_timesteps      | 180224       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0020966814 |\n",
      "|    clip_fraction        | 0.00562      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.48        |\n",
      "|    explained_variance   | -0.000984    |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 17.1         |\n",
      "|    n_updates            | 870          |\n",
      "|    policy_gradient_loss | -0.000613    |\n",
      "|    reward               | 0.1387239    |\n",
      "|    std                  | 1.06         |\n",
      "|    value_loss           | 37.6         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 251          |\n",
      "|    iterations           | 89           |\n",
      "|    time_elapsed         | 724          |\n",
      "|    total_timesteps      | 182272       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0038854266 |\n",
      "|    clip_fraction        | 0.0154       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.48        |\n",
      "|    explained_variance   | 0.00646      |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 16.3         |\n",
      "|    n_updates            | 880          |\n",
      "|    policy_gradient_loss | -0.00167     |\n",
      "|    reward               | 0.77448523   |\n",
      "|    std                  | 1.06         |\n",
      "|    value_loss           | 28.8         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 250          |\n",
      "|    iterations           | 90           |\n",
      "|    time_elapsed         | 735          |\n",
      "|    total_timesteps      | 184320       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0013996528 |\n",
      "|    clip_fraction        | 0.00322      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.48        |\n",
      "|    explained_variance   | 0.0234       |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 19.4         |\n",
      "|    n_updates            | 890          |\n",
      "|    policy_gradient_loss | -9.88e-05    |\n",
      "|    reward               | 3.2562761    |\n",
      "|    std                  | 1.06         |\n",
      "|    value_loss           | 35.3         |\n",
      "------------------------------------------\n",
      "day: 754, episode: 380\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1451466.91\n",
      "total_reward: 451466.91\n",
      "total_cost: 10774.82\n",
      "total_trades: 749\n",
      "Sharpe: 0.611\n",
      "=================================\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 249         |\n",
      "|    iterations           | 91          |\n",
      "|    time_elapsed         | 746         |\n",
      "|    total_timesteps      | 186368      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004282136 |\n",
      "|    clip_fraction        | 0.0123      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.48       |\n",
      "|    explained_variance   | 0.0235      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 17.7        |\n",
      "|    n_updates            | 900         |\n",
      "|    policy_gradient_loss | 0.00123     |\n",
      "|    reward               | 2.985571    |\n",
      "|    std                  | 1.07        |\n",
      "|    value_loss           | 35.2        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 248          |\n",
      "|    iterations           | 92           |\n",
      "|    time_elapsed         | 757          |\n",
      "|    total_timesteps      | 188416       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0034523944 |\n",
      "|    clip_fraction        | 0.0101       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.48        |\n",
      "|    explained_variance   | 0.0361       |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 21.3         |\n",
      "|    n_updates            | 910          |\n",
      "|    policy_gradient_loss | -0.000762    |\n",
      "|    reward               | 0.58422583   |\n",
      "|    std                  | 1.07         |\n",
      "|    value_loss           | 35.6         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 247          |\n",
      "|    iterations           | 93           |\n",
      "|    time_elapsed         | 769          |\n",
      "|    total_timesteps      | 190464       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0027538196 |\n",
      "|    clip_fraction        | 0.0159       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.48        |\n",
      "|    explained_variance   | 0.043        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 15.5         |\n",
      "|    n_updates            | 920          |\n",
      "|    policy_gradient_loss | -0.00178     |\n",
      "|    reward               | 0.9626617    |\n",
      "|    std                  | 1.07         |\n",
      "|    value_loss           | 32.5         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 246         |\n",
      "|    iterations           | 94          |\n",
      "|    time_elapsed         | 780         |\n",
      "|    total_timesteps      | 192512      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005590352 |\n",
      "|    clip_fraction        | 0.0421      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.48       |\n",
      "|    explained_variance   | 0.0534      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 22.3        |\n",
      "|    n_updates            | 930         |\n",
      "|    policy_gradient_loss | -0.00243    |\n",
      "|    reward               | -0.34829792 |\n",
      "|    std                  | 1.07        |\n",
      "|    value_loss           | 33          |\n",
      "-----------------------------------------\n",
      "day: 754, episode: 390\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1400414.85\n",
      "total_reward: 400414.85\n",
      "total_cost: 10776.57\n",
      "total_trades: 750\n",
      "Sharpe: 0.591\n",
      "=================================\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 246          |\n",
      "|    iterations           | 95           |\n",
      "|    time_elapsed         | 790          |\n",
      "|    total_timesteps      | 194560       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0068379906 |\n",
      "|    clip_fraction        | 0.0369       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.49        |\n",
      "|    explained_variance   | 0.0288       |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 18.7         |\n",
      "|    n_updates            | 940          |\n",
      "|    policy_gradient_loss | 0.000636     |\n",
      "|    reward               | 1.2371314    |\n",
      "|    std                  | 1.07         |\n",
      "|    value_loss           | 33.2         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 245         |\n",
      "|    iterations           | 96          |\n",
      "|    time_elapsed         | 800         |\n",
      "|    total_timesteps      | 196608      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002989748 |\n",
      "|    clip_fraction        | 0.0344      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.49       |\n",
      "|    explained_variance   | 0.0284      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 17.5        |\n",
      "|    n_updates            | 950         |\n",
      "|    policy_gradient_loss | 0.00109     |\n",
      "|    reward               | 0.4020406   |\n",
      "|    std                  | 1.07        |\n",
      "|    value_loss           | 26.6        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 245          |\n",
      "|    iterations           | 97           |\n",
      "|    time_elapsed         | 809          |\n",
      "|    total_timesteps      | 198656       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0014940443 |\n",
      "|    clip_fraction        | 0.0105       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.48        |\n",
      "|    explained_variance   | 0.0525       |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 16.1         |\n",
      "|    n_updates            | 960          |\n",
      "|    policy_gradient_loss | -0.00207     |\n",
      "|    reward               | 0.1838675    |\n",
      "|    std                  | 1.07         |\n",
      "|    value_loss           | 29.1         |\n",
      "------------------------------------------\n",
      "day: 754, episode: 400\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1400692.64\n",
      "total_reward: 400692.64\n",
      "total_cost: 11089.88\n",
      "total_trades: 751\n",
      "Sharpe: 0.598\n",
      "=================================\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 245          |\n",
      "|    iterations           | 98           |\n",
      "|    time_elapsed         | 817          |\n",
      "|    total_timesteps      | 200704       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0012525322 |\n",
      "|    clip_fraction        | 0.0105       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.49        |\n",
      "|    explained_variance   | 0.0504       |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 10.6         |\n",
      "|    n_updates            | 970          |\n",
      "|    policy_gradient_loss | -0.000485    |\n",
      "|    reward               | -2.0879688   |\n",
      "|    std                  | 1.07         |\n",
      "|    value_loss           | 22.3         |\n",
      "------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "trained_ppo = agent.train_model(model=model_ppo, \n",
    "                             tb_log_name='ppo',\n",
    "                             total_timesteps=200000) if if_using_ppo else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "C6AidlWyvwzm"
   },
   "outputs": [],
   "source": [
    "trained_ppo.save(TRAINED_MODEL_DIR + \"/agent_ppo\") if if_using_ppo else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3Zpv4S0-fDBv"
   },
   "source": [
    "### Agent 4: TD3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "JSAHhV4Xc-bh"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'batch_size': 100, 'buffer_size': 1000000, 'learning_rate': 0.001}\n",
      "Using cuda device\n",
      "Logging to results/td3\n"
     ]
    }
   ],
   "source": [
    "agent = DRLAgent(env = env_train)\n",
    "TD3_PARAMS = {\"batch_size\": 100, \n",
    "              \"buffer_size\": 1000000, \n",
    "              \"learning_rate\": 0.001}\n",
    "\n",
    "model_td3 = agent.get_model(\"td3\",model_kwargs = TD3_PARAMS)\n",
    "\n",
    "if if_using_td3:\n",
    "  # set up logger\n",
    "  tmp_path = RESULTS_DIR + '/td3'\n",
    "  new_logger_td3 = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n",
    "  # Set new logger\n",
    "  model_td3.set_logger(new_logger_td3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "OSRxNYAxdKpU"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 4        |\n",
      "|    fps             | 174      |\n",
      "|    time_elapsed    | 17       |\n",
      "|    total_timesteps | 3020     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.29e+04 |\n",
      "|    critic_loss     | 1.32e+06 |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 2919     |\n",
      "|    reward          | 0.0      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 8        |\n",
      "|    fps             | 141      |\n",
      "|    time_elapsed    | 42       |\n",
      "|    total_timesteps | 6040     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.18e+04 |\n",
      "|    critic_loss     | 2.6e+05  |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 5939     |\n",
      "|    reward          | 0.0      |\n",
      "---------------------------------\n",
      "day: 754, episode: 410\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1000000.00\n",
      "total_reward: 0.00\n",
      "total_cost: 0.00\n",
      "total_trades: 0\n",
      "=================================\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 12       |\n",
      "|    fps             | 136      |\n",
      "|    time_elapsed    | 66       |\n",
      "|    total_timesteps | 9060     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.23e+04 |\n",
      "|    critic_loss     | 1.11e+05 |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 8959     |\n",
      "|    reward          | 0.0      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 16       |\n",
      "|    fps             | 123      |\n",
      "|    time_elapsed    | 97       |\n",
      "|    total_timesteps | 12080    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.21e+04 |\n",
      "|    critic_loss     | 1.03e+04 |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 11979    |\n",
      "|    reward          | 0.0      |\n",
      "---------------------------------\n",
      "day: 754, episode: 420\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1000000.00\n",
      "total_reward: 0.00\n",
      "total_cost: 0.00\n",
      "total_trades: 0\n",
      "=================================\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 20       |\n",
      "|    fps             | 119      |\n",
      "|    time_elapsed    | 126      |\n",
      "|    total_timesteps | 15100    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.38e+04 |\n",
      "|    critic_loss     | 4.53e+03 |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 14999    |\n",
      "|    reward          | 0.0      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 24       |\n",
      "|    fps             | 127      |\n",
      "|    time_elapsed    | 142      |\n",
      "|    total_timesteps | 18120    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.27e+04 |\n",
      "|    critic_loss     | 4.68e+03 |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 18019    |\n",
      "|    reward          | 0.0      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 28       |\n",
      "|    fps             | 126      |\n",
      "|    time_elapsed    | 167      |\n",
      "|    total_timesteps | 21140    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.16e+04 |\n",
      "|    critic_loss     | 1.11e+03 |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 21039    |\n",
      "|    reward          | 0.0      |\n",
      "---------------------------------\n",
      "day: 754, episode: 430\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1000000.00\n",
      "total_reward: 0.00\n",
      "total_cost: 0.00\n",
      "total_trades: 0\n",
      "=================================\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 32       |\n",
      "|    fps             | 127      |\n",
      "|    time_elapsed    | 189      |\n",
      "|    total_timesteps | 24160    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.05e+04 |\n",
      "|    critic_loss     | 1.28e+04 |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 24059    |\n",
      "|    reward          | 0.0      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 36       |\n",
      "|    fps             | 125      |\n",
      "|    time_elapsed    | 216      |\n",
      "|    total_timesteps | 27180    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 9.65e+03 |\n",
      "|    critic_loss     | 1.59e+04 |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 27079    |\n",
      "|    reward          | 0.0      |\n",
      "---------------------------------\n",
      "day: 754, episode: 440\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1000000.00\n",
      "total_reward: 0.00\n",
      "total_cost: 0.00\n",
      "total_trades: 0\n",
      "=================================\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 40       |\n",
      "|    fps             | 129      |\n",
      "|    time_elapsed    | 233      |\n",
      "|    total_timesteps | 30200    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 8.87e+03 |\n",
      "|    critic_loss     | 1.58e+06 |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 30099    |\n",
      "|    reward          | 0.0      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 44       |\n",
      "|    fps             | 129      |\n",
      "|    time_elapsed    | 256      |\n",
      "|    total_timesteps | 33220    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 8.14e+03 |\n",
      "|    critic_loss     | 324      |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 33119    |\n",
      "|    reward          | 0.0      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 48       |\n",
      "|    fps             | 131      |\n",
      "|    time_elapsed    | 276      |\n",
      "|    total_timesteps | 36240    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 7.63e+03 |\n",
      "|    critic_loss     | 1.16e+06 |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 36139    |\n",
      "|    reward          | 0.0      |\n",
      "---------------------------------\n",
      "day: 754, episode: 450\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1000000.00\n",
      "total_reward: 0.00\n",
      "total_cost: 0.00\n",
      "total_trades: 0\n",
      "=================================\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 52       |\n",
      "|    fps             | 129      |\n",
      "|    time_elapsed    | 302      |\n",
      "|    total_timesteps | 39260    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 7.61e+03 |\n",
      "|    critic_loss     | 1.15e+06 |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 39159    |\n",
      "|    reward          | 0.0      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 56       |\n",
      "|    fps             | 127      |\n",
      "|    time_elapsed    | 331      |\n",
      "|    total_timesteps | 42280    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 6.85e+03 |\n",
      "|    critic_loss     | 1.32e+04 |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 42179    |\n",
      "|    reward          | 0.0      |\n",
      "---------------------------------\n",
      "day: 754, episode: 460\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1000000.00\n",
      "total_reward: 0.00\n",
      "total_cost: 0.00\n",
      "total_trades: 0\n",
      "=================================\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 60       |\n",
      "|    fps             | 124      |\n",
      "|    time_elapsed    | 365      |\n",
      "|    total_timesteps | 45300    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 6.44e+03 |\n",
      "|    critic_loss     | 3.98e+03 |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 45199    |\n",
      "|    reward          | 0.0      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 64       |\n",
      "|    fps             | 120      |\n",
      "|    time_elapsed    | 399      |\n",
      "|    total_timesteps | 48320    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 5.91e+03 |\n",
      "|    critic_loss     | 6.99e+05 |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 48219    |\n",
      "|    reward          | 0.0      |\n",
      "---------------------------------\n"
     ]
    }
   ],
   "source": [
    "trained_td3 = agent.train_model(model=model_td3, \n",
    "                             tb_log_name='td3',\n",
    "                             total_timesteps=50000) if if_using_td3 else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "OkJV6V_mv2hw"
   },
   "outputs": [],
   "source": [
    "trained_td3.save(TRAINED_MODEL_DIR + \"/agent_td3\") if if_using_td3 else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dr49PotrfG01"
   },
   "source": [
    "### Agent 5: SAC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "xwOhVjqRkCdM"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'batch_size': 128, 'buffer_size': 100000, 'learning_rate': 0.0001, 'learning_starts': 100, 'ent_coef': 'auto_0.1'}\n",
      "Using cuda device\n",
      "Logging to results/sac\n"
     ]
    }
   ],
   "source": [
    "agent = DRLAgent(env = env_train)\n",
    "SAC_PARAMS = {\n",
    "    \"batch_size\": 128,\n",
    "    \"buffer_size\": 100000,\n",
    "    \"learning_rate\": 0.0001,\n",
    "    \"learning_starts\": 100,\n",
    "    \"ent_coef\": \"auto_0.1\",\n",
    "}\n",
    "\n",
    "model_sac = agent.get_model(\"sac\",model_kwargs = SAC_PARAMS)\n",
    "\n",
    "if if_using_sac:\n",
    "  # set up logger\n",
    "  tmp_path = RESULTS_DIR + '/sac'\n",
    "  new_logger_sac = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n",
    "  # Set new logger\n",
    "  model_sac.set_logger(new_logger_sac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "K8RSdKCckJyH"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "day: 754, episode: 470\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1000000.00\n",
      "total_reward: 0.00\n",
      "total_cost: 0.00\n",
      "total_trades: 0\n",
      "=================================\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 4         |\n",
      "|    fps             | 53        |\n",
      "|    time_elapsed    | 55        |\n",
      "|    total_timesteps | 3020      |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.16e+04 |\n",
      "|    critic_loss     | 1.45e+04  |\n",
      "|    ent_coef        | 0.134     |\n",
      "|    ent_coef_loss   | 18.8      |\n",
      "|    learning_rate   | 0.0001    |\n",
      "|    n_updates       | 2919      |\n",
      "|    reward          | 0.0       |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 8         |\n",
      "|    fps             | 58        |\n",
      "|    time_elapsed    | 102       |\n",
      "|    total_timesteps | 6040      |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -9.88e+03 |\n",
      "|    critic_loss     | 3.18e+03  |\n",
      "|    ent_coef        | 0.181     |\n",
      "|    ent_coef_loss   | 16.1      |\n",
      "|    learning_rate   | 0.0001    |\n",
      "|    n_updates       | 5939      |\n",
      "|    reward          | 0.0       |\n",
      "----------------------------------\n",
      "day: 754, episode: 480\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1000000.00\n",
      "total_reward: 0.00\n",
      "total_cost: 0.00\n",
      "total_trades: 0\n",
      "=================================\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 12        |\n",
      "|    fps             | 61        |\n",
      "|    time_elapsed    | 146       |\n",
      "|    total_timesteps | 9060      |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -7.67e+03 |\n",
      "|    critic_loss     | 3.41e+03  |\n",
      "|    ent_coef        | 0.245     |\n",
      "|    ent_coef_loss   | 13.2      |\n",
      "|    learning_rate   | 0.0001    |\n",
      "|    n_updates       | 8959      |\n",
      "|    reward          | 0.0       |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 16        |\n",
      "|    fps             | 58        |\n",
      "|    time_elapsed    | 205       |\n",
      "|    total_timesteps | 12080     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -5.95e+03 |\n",
      "|    critic_loss     | 3.52e+05  |\n",
      "|    ent_coef        | 0.331     |\n",
      "|    ent_coef_loss   | 10.4      |\n",
      "|    learning_rate   | 0.0001    |\n",
      "|    n_updates       | 11979     |\n",
      "|    reward          | 0.0       |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 20        |\n",
      "|    fps             | 57        |\n",
      "|    time_elapsed    | 264       |\n",
      "|    total_timesteps | 15100     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -5.42e+03 |\n",
      "|    critic_loss     | 2.84e+03  |\n",
      "|    ent_coef        | 0.448     |\n",
      "|    ent_coef_loss   | 7.5       |\n",
      "|    learning_rate   | 0.0001    |\n",
      "|    n_updates       | 14999     |\n",
      "|    reward          | 0.0       |\n",
      "----------------------------------\n",
      "day: 754, episode: 490\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1000000.00\n",
      "total_reward: 0.00\n",
      "total_cost: 0.00\n",
      "total_trades: 0\n",
      "=================================\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 24        |\n",
      "|    fps             | 57        |\n",
      "|    time_elapsed    | 316       |\n",
      "|    total_timesteps | 18120     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -4.67e+03 |\n",
      "|    critic_loss     | 1.87e+05  |\n",
      "|    ent_coef        | 0.606     |\n",
      "|    ent_coef_loss   | 4.7       |\n",
      "|    learning_rate   | 0.0001    |\n",
      "|    n_updates       | 18019     |\n",
      "|    reward          | 0.0       |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 28        |\n",
      "|    fps             | 55        |\n",
      "|    time_elapsed    | 380       |\n",
      "|    total_timesteps | 21140     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -3.58e+03 |\n",
      "|    critic_loss     | 89.4      |\n",
      "|    ent_coef        | 0.82      |\n",
      "|    ent_coef_loss   | 1.86      |\n",
      "|    learning_rate   | 0.0001    |\n",
      "|    n_updates       | 21039     |\n",
      "|    reward          | 0.0       |\n",
      "----------------------------------\n",
      "day: 754, episode: 500\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1000000.00\n",
      "total_reward: 0.00\n",
      "total_cost: 0.00\n",
      "total_trades: 0\n",
      "=================================\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 32        |\n",
      "|    fps             | 54        |\n",
      "|    time_elapsed    | 447       |\n",
      "|    total_timesteps | 24160     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -2.86e+03 |\n",
      "|    critic_loss     | 3.57e+04  |\n",
      "|    ent_coef        | 1.11      |\n",
      "|    ent_coef_loss   | -0.961    |\n",
      "|    learning_rate   | 0.0001    |\n",
      "|    n_updates       | 24059     |\n",
      "|    reward          | 0.0       |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 36        |\n",
      "|    fps             | 54        |\n",
      "|    time_elapsed    | 495       |\n",
      "|    total_timesteps | 27180     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -2.09e+03 |\n",
      "|    critic_loss     | 795       |\n",
      "|    ent_coef        | 1.5       |\n",
      "|    ent_coef_loss   | -3.83     |\n",
      "|    learning_rate   | 0.0001    |\n",
      "|    n_updates       | 27079     |\n",
      "|    reward          | 0.0       |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 40        |\n",
      "|    fps             | 54        |\n",
      "|    time_elapsed    | 549       |\n",
      "|    total_timesteps | 30200     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.32e+03 |\n",
      "|    critic_loss     | 2.72e+03  |\n",
      "|    ent_coef        | 2.03      |\n",
      "|    ent_coef_loss   | -6.69     |\n",
      "|    learning_rate   | 0.0001    |\n",
      "|    n_updates       | 30099     |\n",
      "|    reward          | 0.0       |\n",
      "----------------------------------\n",
      "day: 754, episode: 510\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1000000.00\n",
      "total_reward: 0.00\n",
      "total_cost: 0.00\n",
      "total_trades: 0\n",
      "=================================\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 44       |\n",
      "|    fps             | 54       |\n",
      "|    time_elapsed    | 610      |\n",
      "|    total_timesteps | 33220    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -721     |\n",
      "|    critic_loss     | 1.02e+04 |\n",
      "|    ent_coef        | 2.74     |\n",
      "|    ent_coef_loss   | -9.56    |\n",
      "|    learning_rate   | 0.0001   |\n",
      "|    n_updates       | 33119    |\n",
      "|    reward          | 0.0      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 48       |\n",
      "|    fps             | 54       |\n",
      "|    time_elapsed    | 662      |\n",
      "|    total_timesteps | 36240    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -198     |\n",
      "|    critic_loss     | 3.82e+03 |\n",
      "|    ent_coef        | 3.71     |\n",
      "|    ent_coef_loss   | -12.4    |\n",
      "|    learning_rate   | 0.0001   |\n",
      "|    n_updates       | 36139    |\n",
      "|    reward          | 0.0      |\n",
      "---------------------------------\n",
      "day: 754, episode: 520\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1000000.00\n",
      "total_reward: 0.00\n",
      "total_cost: 0.00\n",
      "total_trades: 0\n",
      "=================================\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 52       |\n",
      "|    fps             | 53       |\n",
      "|    time_elapsed    | 728      |\n",
      "|    total_timesteps | 39260    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 515      |\n",
      "|    critic_loss     | 464      |\n",
      "|    ent_coef        | 5.02     |\n",
      "|    ent_coef_loss   | -15.2    |\n",
      "|    learning_rate   | 0.0001   |\n",
      "|    n_updates       | 39159    |\n",
      "|    reward          | 0.0      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 56       |\n",
      "|    fps             | 54       |\n",
      "|    time_elapsed    | 776      |\n",
      "|    total_timesteps | 42280    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.31e+03 |\n",
      "|    critic_loss     | 2.27e+03 |\n",
      "|    ent_coef        | 6.79     |\n",
      "|    ent_coef_loss   | -17.7    |\n",
      "|    learning_rate   | 0.0001   |\n",
      "|    n_updates       | 42179    |\n",
      "|    reward          | 0.0      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 60       |\n",
      "|    fps             | 55       |\n",
      "|    time_elapsed    | 821      |\n",
      "|    total_timesteps | 45300    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.17e+03 |\n",
      "|    critic_loss     | 4.14e+04 |\n",
      "|    ent_coef        | 9.18     |\n",
      "|    ent_coef_loss   | -20.9    |\n",
      "|    learning_rate   | 0.0001   |\n",
      "|    n_updates       | 45199    |\n",
      "|    reward          | 0.0      |\n",
      "---------------------------------\n",
      "day: 754, episode: 530\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1000000.00\n",
      "total_reward: 0.00\n",
      "total_cost: 0.00\n",
      "total_trades: 0\n",
      "=================================\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 64       |\n",
      "|    fps             | 55       |\n",
      "|    time_elapsed    | 877      |\n",
      "|    total_timesteps | 48320    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 3.74e+03 |\n",
      "|    critic_loss     | 3.44e+04 |\n",
      "|    ent_coef        | 12.4     |\n",
      "|    ent_coef_loss   | -23.5    |\n",
      "|    learning_rate   | 0.0001   |\n",
      "|    n_updates       | 48219    |\n",
      "|    reward          | 0.0      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 68       |\n",
      "|    fps             | 55       |\n",
      "|    time_elapsed    | 925      |\n",
      "|    total_timesteps | 51340    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 5.15e+03 |\n",
      "|    critic_loss     | 2.22e+05 |\n",
      "|    ent_coef        | 16.8     |\n",
      "|    ent_coef_loss   | -26.7    |\n",
      "|    learning_rate   | 0.0001   |\n",
      "|    n_updates       | 51239    |\n",
      "|    reward          | 0.0      |\n",
      "---------------------------------\n",
      "day: 754, episode: 540\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1000000.00\n",
      "total_reward: 0.00\n",
      "total_cost: 0.00\n",
      "total_trades: 0\n",
      "=================================\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 72       |\n",
      "|    fps             | 57       |\n",
      "|    time_elapsed    | 949      |\n",
      "|    total_timesteps | 54360    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 7.36e+03 |\n",
      "|    critic_loss     | 1.96e+04 |\n",
      "|    ent_coef        | 22.7     |\n",
      "|    ent_coef_loss   | -29.3    |\n",
      "|    learning_rate   | 0.0001   |\n",
      "|    n_updates       | 54259    |\n",
      "|    reward          | 0.0      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 76       |\n",
      "|    fps             | 58       |\n",
      "|    time_elapsed    | 974      |\n",
      "|    total_timesteps | 57380    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.01e+04 |\n",
      "|    critic_loss     | 2.65e+03 |\n",
      "|    ent_coef        | 30.7     |\n",
      "|    ent_coef_loss   | -32.3    |\n",
      "|    learning_rate   | 0.0001   |\n",
      "|    n_updates       | 57279    |\n",
      "|    reward          | 0.0      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 80       |\n",
      "|    fps             | 60       |\n",
      "|    time_elapsed    | 1000     |\n",
      "|    total_timesteps | 60400    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.34e+04 |\n",
      "|    critic_loss     | 6.59e+04 |\n",
      "|    ent_coef        | 41.6     |\n",
      "|    ent_coef_loss   | -34.9    |\n",
      "|    learning_rate   | 0.0001   |\n",
      "|    n_updates       | 60299    |\n",
      "|    reward          | 0.0      |\n",
      "---------------------------------\n",
      "day: 754, episode: 550\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1000000.00\n",
      "total_reward: 0.00\n",
      "total_cost: 0.00\n",
      "total_trades: 0\n",
      "=================================\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 84       |\n",
      "|    fps             | 61       |\n",
      "|    time_elapsed    | 1026     |\n",
      "|    total_timesteps | 63420    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.87e+04 |\n",
      "|    critic_loss     | 3.81e+04 |\n",
      "|    ent_coef        | 56.2     |\n",
      "|    ent_coef_loss   | -38.2    |\n",
      "|    learning_rate   | 0.0001   |\n",
      "|    n_updates       | 63319    |\n",
      "|    reward          | 0.0      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 88       |\n",
      "|    fps             | 62       |\n",
      "|    time_elapsed    | 1056     |\n",
      "|    total_timesteps | 66440    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.53e+04 |\n",
      "|    critic_loss     | 1.56e+04 |\n",
      "|    ent_coef        | 76       |\n",
      "|    ent_coef_loss   | -40.9    |\n",
      "|    learning_rate   | 0.0001   |\n",
      "|    n_updates       | 66339    |\n",
      "|    reward          | 0.0      |\n",
      "---------------------------------\n",
      "day: 754, episode: 560\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1000000.00\n",
      "total_reward: 0.00\n",
      "total_cost: 0.00\n",
      "total_trades: 0\n",
      "=================================\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 92       |\n",
      "|    fps             | 64       |\n",
      "|    time_elapsed    | 1084     |\n",
      "|    total_timesteps | 69460    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 3.41e+04 |\n",
      "|    critic_loss     | 2.78e+04 |\n",
      "|    ent_coef        | 103      |\n",
      "|    ent_coef_loss   | -43.5    |\n",
      "|    learning_rate   | 0.0001   |\n",
      "|    n_updates       | 69359    |\n",
      "|    reward          | 0.0      |\n",
      "---------------------------------\n"
     ]
    }
   ],
   "source": [
    "trained_sac = agent.train_model(model=model_sac, \n",
    "                             tb_log_name='sac',\n",
    "                             total_timesteps=70000) if if_using_sac else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "_SpZoQgPv7GO"
   },
   "outputs": [],
   "source": [
    "trained_sac.save(TRAINED_MODEL_DIR + \"/agent_sac\") if if_using_sac else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PgGm3dQZfRks"
   },
   "source": [
    "## Save the trained agent\n",
    "Trained agents should have already been saved in the \"trained_models\" drectory after you run the code blocks above.\n",
    "\n",
    "For Colab users, the zip files should be at \"./trained_models\" or \"/content/trained_models\".\n",
    "\n",
    "For users running on your local environment, the zip files should be at \"./trained_models\"."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "MRiOtrywfAo1",
    "_gDkU-j-fCmZ",
    "3Zpv4S0-fDBv",
    "Dr49PotrfG01"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "trading",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
